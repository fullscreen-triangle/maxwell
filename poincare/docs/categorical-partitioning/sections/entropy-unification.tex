\section{Entropy Unification: The Equivalence Theorem}
\label{sec:unification}

We have derived entropy from three independent starting points, each proceeding from distinct axioms and employing different mathematical machinery:
\begin{align}
    \text{Oscillatory mechanics (Section~\ref{sec:oscillatory}):} \quad & \Sosc = \kB M \ln n \\
    \text{Categorical mechanics (Section~\ref{sec:categorical}):} \quad & \Scat = \kB M \ln n \\
    \text{Partition mechanics (Section~\ref{sec:partition}):} \quad & \Spart = \kB M \ln n
\end{align}

The mathematical identity of these three formulas is not coincidental. In this section, we prove that oscillation, category, and partition are not merely analogous but fundamentally equivalent descriptions of the same underlying structure. This equivalence establishes that thermodynamic entropy has a unique mathematical form—the formula $S = \kB M \ln n$ is not one possible expression among many but the canonical expression of a fundamental physical invariant.

\subsection{The Unified Entropy Formula}

We begin by establishing the formal equivalence of the three entropy expressions through careful identification of their parameters and structural elements.

\begin{theorem}[Entropy Equivalence]
\label{thm:equivalence}
The three entropy formulas are mathematically identical:
\begin{equation}
    \boxed{\Sosc = \Scat = \Spart = S = \kB M \ln n}
\end{equation}
where the parameters $M$ and $n$ have consistent interpretations across all three frameworks, and the equality holds not merely numerically but structurally—the three expressions describe the same physical quantity computed in three different ways.
\end{theorem}

\begin{proof}
We establish the equivalence by demonstrating that the parameters $(M, n)$ appearing in the three formulas are not merely numerically equal but represent the same physical quantities. The proof proceeds in three steps: identifying $n$, identifying $M$, and verifying that the counting arguments are identical.

\textbf{Step 1: Identification of the branching parameter $n$.}

The parameter $n$ appears in all three derivations as the number of distinguishable alternatives at each level of structure:

\begin{itemize}
    \item \textbf{Oscillatory mechanics:} $n$ is the number of distinguishable quantum states per oscillatory mode (Definition~\ref{def:quantum_states}). For a quantum harmonic oscillator at temperature $T$, the accessible states are $\{|0\rangle, |1\rangle, \ldots, |n-1\rangle\}$ with $n \approx \kB T / \hbar \omega$ in the high-temperature limit. Each state $|k\rangle$ is characterized by a distinct energy $E_k = \hbar \omega (k + 1/2)$ and a distinct wavefunction $\psi_k(x)$, making the states operationally distinguishable through energy measurements or wavefunction observations.
    
    \item \textbf{Categorical mechanics:} $n$ is the number of distinguishable levels per categorical dimension (Axiom~\ref{axiom:resolution}). For a categorical dimension $\mathcal{C}_i$, the levels are $\{c_1, c_2, \ldots, c_n\}$, where each level $c_j$ can be distinguished from every other level $c_k$ ($k \neq j$) by at least one observable $\mathcal{O}$ such that $\mathcal{O}(c_j) \neq \mathcal{O}(c_k)$. The distinguishability is operational—there exists a measurement procedure that can determine which level the system occupies.
    
    \item \textbf{Partition mechanics:} $n$ is the branching factor per partition operation (Axiom~\ref{axiom:branching}). Each partition divides a parent system into $n$ child subsystems $\{X_1, X_2, \ldots, X_n\}$, where the subsystems are disjoint ($X_i \cap X_j = \emptyset$ for $i \neq j$) and exhaustive ($\bigcup_{i=1}^n X_i = X$). The distinguishability of the subsystems is guaranteed by the disjointness condition—each element of the system belongs to exactly one subsystem.
\end{itemize}

These three definitions describe the same quantity: the number of distinguishable alternatives available at each step of a hierarchical structure. Whether we call this a ``quantum state,'' a ``categorical level,'' or a ``partition branch'' is a matter of linguistic convention—the mathematical structure is identical. In all three cases, $n$ counts the number of ways to make a single independent choice, and the distinguishability is operational (based on measurements or observations).

\textbf{Step 2: Identification of the depth parameter $M$.}

The parameter $M$ appears in all three derivations as the number of independent levels in the hierarchical structure:

\begin{itemize}
    \item \textbf{Oscillatory mechanics:} $M$ is the number of independent oscillatory modes (Definition~\ref{def:mode}). For a system with $M$ modes, the state is specified by $M$ quantum numbers $(n_1, n_2, \ldots, n_M)$, one for each mode. The independence of modes means that the energy is additive: $E_{\text{total}} = \sum_{i=1}^M E_i(n_i)$, and the quantum numbers can be chosen independently without constraint (except for the total energy constraint in the microcanonical ensemble).
    
    \item \textbf{Categorical mechanics:} $M$ is the number of orthogonal categorical dimensions (Axiom~\ref{axiom:dimensional}). The categorical space decomposes as $\mathcal{C} = \mathcal{C}_1 \times \mathcal{C}_2 \times \cdots \times \mathcal{C}_M$, where the Cartesian product structure ensures that distinctions along dimension $i$ are independent of distinctions along dimension $j$ for $i \neq j$. A categorical state is specified by $M$ coordinates $(c_1, c_2, \ldots, c_M)$, one for each dimension.
    
    \item \textbf{Partition mechanics:} $M$ is the depth of recursive partitioning (Definition~\ref{def:partition_tree}). A partition tree of depth $M$ has $M$ levels (excluding the root), and a path from root to leaf is specified by $M$ branch choices $(h_1, h_2, \ldots, h_M)$, one for each level. The independence of levels means that the choice at level $j$ does not constrain the choices at other levels (by Axiom~\ref{axiom:recursive_part}, the partition structure is the same at all levels).
\end{itemize}

These three definitions describe the same quantity: the number of independent degrees of freedom over which distinctions can be made. The independence is structural—the choices at different levels or along different dimensions can be made without mutual constraint. In all three cases, $M$ counts the dimensionality of the configuration space in the sense of the number of independent coordinates required to specify a state.

\textbf{Step 3: Verification of identical counting arguments.}

In all three derivations, the total number of distinguishable configurations is computed as:
\begin{equation}
    W = n^M
\end{equation}

This follows from the fundamental principle of combinatorics: when making $M$ independent choices with $n$ options each, the total number of possible outcomes is the product $\underbrace{n \times n \times \cdots \times n}_{M \text{ factors}} = n^M$. The three derivations apply this principle in different contexts:

\begin{itemize}
    \item \textbf{Oscillatory:} $W_{\text{osc}} = n^M$ counts the number of ways to assign quantum numbers $(n_1, n_2, \ldots, n_M)$ to $M$ modes, with each $n_i \in \{0, 1, \ldots, n-1\}$. This is the dimension of the Hilbert space spanned by the accessible states (or the volume of the accessible region of phase space in the classical limit).
    
    \item \textbf{Categorical:} $|\mathcal{C}| = n^M$ counts the number of categorical states in the Cartesian product $\mathcal{C} = \mathcal{C}_1 \times \cdots \times \mathcal{C}_M$, where each factor space has cardinality $|\mathcal{C}_i| = n$. This is the cardinality of the categorical state space (Theorem~\ref{thm:cardinality}).
    
    \item \textbf{Partition:} $P = n^M$ counts the number of leaf nodes in a partition tree of depth $M$ with branching factor $n$ (Theorem~\ref{thm:leaf_nodes}). Equivalently, it counts the number of distinct paths from root to leaf (Theorem~\ref{thm:partition_paths}).
\end{itemize}

The three counting arguments are mathematically identical—they all compute the cardinality of a set with product structure $\underbrace{\{1, 2, \ldots, n\} \times \{1, 2, \ldots, n\} \times \cdots \times \{1, 2, \ldots, n\}}_{M \text{ factors}}$.

\textbf{Step 4: Application of Boltzmann's relation.}

In all three derivations, the entropy is obtained by applying Boltzmann's fundamental relation:
\begin{equation}
    S = \kB \ln W
\end{equation}

Substituting $W = n^M$:
\begin{equation}
    S = \kB \ln(n^M) = \kB M \ln n
\end{equation}

This calculation is identical in all three frameworks. The three entropy formulas $\Sosc$, $\Scat$, and $\Spart$ are therefore not merely numerically equal but are three different names for the same mathematical expression applied to the same physical quantity.

\textbf{Conclusion:} The parameters $(M, n)$ have identical meanings across all three frameworks, the counting arguments are structurally identical, and the application of Boltzmann's relation is identical. Therefore:
\begin{equation}
    \Sosc = \Scat = \Spart = \kB M \ln n
\end{equation}

The equality is not approximate or coincidental but exact and structural. The three derivations compute the same entropy in three different ways, and the convergence to a single formula demonstrates that oscillation, category, and partition are three perspectives on a single underlying structure.
\end{proof}

\subsection{Physical Meaning of the Equivalence}

The mathematical equivalence established in Theorem~\ref{thm:equivalence} has profound physical implications. We now demonstrate that the three frameworks are not merely analogous but are different descriptions of the same physical reality. The equivalence is not a formal coincidence but reflects a deep structural unity.

\begin{theorem}[Oscillation-Category Equivalence]
\label{thm:osc_cat}
An oscillatory mode IS a categorical dimension. The number of quantum states accessible to a mode equals the number of distinguishable levels in the corresponding categorical dimension. The two concepts are not analogous but identical—they are the same structure described in different languages.
\end{theorem}

\begin{proof}
Consider an oscillatory mode with frequency $\omega$ at temperature $T$. By quantum mechanics, the mode admits discrete energy eigenstates $\{|0\rangle, |1\rangle, |2\rangle, \ldots\}$ with energies $E_n = \hbar \omega (n + 1/2)$. The thermally accessible states are those with $E_n \lesssim \kB T$, giving $n \lesssim \kB T / \hbar \omega$. Let $n_{\max}$ denote the maximum accessible quantum number, so the accessible states are $\{|0\rangle, |1\rangle, \ldots, |n_{\max}\rangle\}$.

Each quantum state $|n\rangle$ is distinguishable from every other state $|m\rangle$ for $m \neq n$ in multiple ways:
\begin{itemize}
    \item \textbf{Energy distinguishability:} The states have different energies $E_n \neq E_m$, so an energy measurement can determine which state is occupied.
    \item \textbf{Wavefunction distinguishability:} The states have different spatial wavefunctions $\psi_n(x) \neq \psi_m(x)$, so a position measurement has different probability distributions.
    \item \textbf{Orthogonality:} The states are orthogonal in Hilbert space: $\langle n | m \rangle = \delta_{nm}$, ensuring that they are maximally distinguishable in the quantum mechanical sense.
\end{itemize}

This distinguishability is precisely the defining property of categorical distinctions (Axiom~\ref{axiom:distinguishable}): two states are categorically distinct if and only if there exists an observable $\mathcal{O}$ such that $\mathcal{O}(|n\rangle) \neq \mathcal{O}(|m\rangle)$. For quantum states, the energy operator $\hat{H}$ serves as such an observable: $\hat{H}|n\rangle = E_n |n\rangle$ with $E_n \neq E_m$ for $n \neq m$.

The set of quantum states $\{|0\rangle, |1\rangle, \ldots, |n_{\max}\rangle\}$ therefore forms a categorical dimension with $n_{\max} + 1$ levels. The correspondence is:
\begin{equation}
    \text{Quantum state } |n\rangle \quad \longleftrightarrow \quad \text{Categorical level } c_n
\end{equation}

This correspondence is not an analogy or a mapping between different structures—it is an identification. The oscillatory mode and the categorical dimension are the same structure. The quantum state label $n$ and the categorical level label $c$ are two names for the same thing. The oscillatory framework emphasizes the dynamical aspect (the mode oscillates in time), while the categorical framework emphasizes the structural aspect (the states form a discrete hierarchy), but the underlying physical reality is identical.

To make this concrete, consider a specific example: a diatomic molecule vibrating at frequency $\omega = 10^{14}$ rad/s at temperature $T = 300$ K. The thermal energy is $\kB T \approx 4.14 \times 10^{-21}$ J, and the quantum energy spacing is $\hbar \omega \approx 1.05 \times 10^{-20}$ J. The maximum quantum number is $n_{\max} \approx \kB T / \hbar \omega \approx 0.4$, so only the ground state $|0\rangle$ and possibly the first excited state $|1\rangle$ are thermally accessible.

From the oscillatory perspective, we say the molecule has one vibrational mode with two accessible quantum states. From the categorical perspective, we say the molecule has one categorical dimension with two distinguishable levels (ground and excited). These are two descriptions of the same physical situation. The entropy is $S = \kB \ln 2$ in both cases.
\end{proof}

\begin{theorem}[Category-Partition Equivalence]
\label{thm:cat_part}
A categorical dimension IS a partition level. The number of distinguishable levels in a categorical dimension equals the branching factor of the corresponding partition operation. Making a categorical distinction is the same process as performing a partition.
\end{theorem}

\begin{proof}
Consider a categorical dimension $\mathcal{C}_i$ with $n$ distinguishable levels $\{c_1, c_2, \ldots, c_n\}$. By Axiom~\ref{axiom:distinguishable}, each level $c_j$ can be distinguished from every other level $c_k$ ($k \neq j$) by at least one observable. This distinguishability means that we can operationally determine which level the system occupies through measurement.

The process of determining the occupied level is precisely a partition operation. Before the measurement, the system could be in any of the $n$ levels—the state space is the undivided set $\{c_1, c_2, \ldots, c_n\}$. The measurement partitions this set into $n$ subsets, each containing a single level:
\begin{equation}
    \{c_1, c_2, \ldots, c_n\} = \{c_1\} \cup \{c_2\} \cup \cdots \cup \{c_n\}
\end{equation}

This partition satisfies all three conditions of Axiom~\ref{axiom:partition_exist}:
\begin{itemize}
    \item \textbf{Disjointness:} $\{c_i\} \cap \{c_j\} = \emptyset$ for $i \neq j$ (each level is distinct)
    \item \textbf{Exhaustiveness:} $\bigcup_{i=1}^n \{c_i\} = \{c_1, \ldots, c_n\}$ (all levels are included)
    \item \textbf{Non-triviality:} Each subset $\{c_i\}$ is non-empty (contains one level)
\end{itemize}

The branching factor of this partition is $n$—the undivided set is divided into $n$ parts. This equals the number of categorical levels by construction.

Conversely, consider a partition operation that divides a system $X$ into $n$ subsystems $\{X_1, X_2, \ldots, X_n\}$. Each subsystem is distinguishable from the others (by the disjointness condition). The set of subsystems $\{X_1, X_2, \ldots, X_n\}$ therefore forms a categorical dimension with $n$ levels. The correspondence is:
\begin{equation}
    \text{Partition subsystem } X_i \quad \longleftrightarrow \quad \text{Categorical level } c_i
\end{equation}

Again, this is not an analogy but an identification. The partition operation and the categorical distinction are the same process viewed from different angles. The partition framework emphasizes the spatial or structural aspect (dividing a whole into parts), while the categorical framework emphasizes the informational aspect (distinguishing one state from others), but the underlying operation is identical.

To make this concrete, consider a box divided into three chambers by two partitions. From the partition perspective, we say the box has been partitioned into three subsystems (chambers). From the categorical perspective, we say the box has one categorical dimension (chamber location) with three distinguishable levels (left, center, right). A particle in the box occupies one of three partition subsystems, or equivalently, occupies one of three categorical levels. These are two descriptions of the same physical situation.
\end{proof}

\begin{theorem}[Oscillation-Partition Equivalence]
\label{thm:osc_part}
An oscillatory transition IS a partition operation. Changing the quantum number of a mode partitions the system's history into ``before the transition'' and ``after the transition,'' creating a boundary in time that is structurally identical to a spatial partition boundary.
\end{theorem}

\begin{proof}
Consider an oscillatory mode transitioning from quantum state $|n\rangle$ to state $|n'\rangle$ at time $t_0$. This transition has the following properties:

\begin{enumerate}
    \item \textbf{Creates a distinction:} Before time $t_0$, the mode was in state $|n\rangle$; after time $t_0$, the mode is in state $|n'\rangle$. The states $|n\rangle$ and $|n'\rangle$ are distinguishable (they have different energies, different wavefunctions, etc.), so the transition creates a distinction between the pre-transition configuration and the post-transition configuration.
    
    \item \textbf{Divides history:} The transition divides the system's history into two disjoint, exhaustive parts:
    \begin{align}
        H_{\text{before}} &= \{t : t < t_0\} \quad \text{(pre-transition history)} \\
        H_{\text{after}} &= \{t : t \geq t_0\} \quad \text{(post-transition history)}
    \end{align}
    These two sets satisfy the partition conditions: $H_{\text{before}} \cap H_{\text{after}} = \emptyset$ (disjointness), $H_{\text{before}} \cup H_{\text{after}} = \mathbb{R}$ (exhaustiveness), and both sets are non-empty (non-triviality).
    
    \item \textbf{Is irreversible in the thermodynamic sense:} While the quantum state can return to $|n\rangle$ at a later time (the dynamics is reversible), the fact that the transition occurred at time $t_0$ becomes part of the system's history and cannot be erased. The history $H = (|n\rangle \to |n'\rangle \to |n\rangle)$ is distinguishable from the history $H' = (|n\rangle \to |n\rangle \to |n\rangle)$ even though both end in the same state $|n\rangle$.
    
    \item \textbf{Creates a boundary:} The transition time $t_0$ acts as a boundary in time, analogous to how a spatial partition creates a boundary in space. Events before $t_0$ are separated from events after $t_0$ by this temporal boundary.
\end{enumerate}

This structure is precisely that of a partition operation. The undivided whole (the system's history) is divided into two parts (before and after the transition) by a boundary (the transition time). The branching factor is $n = 2$ for a single transition, but if the mode can transition to any of $n$ different states, the branching factor is $n$.

More generally, a sequence of $M$ transitions partitions the history into $M+1$ epochs, creating a partition tree of depth $M$ with temporal boundaries at the transition times. This is structurally identical to a spatial partition tree with spatial boundaries at the partition interfaces.

The correspondence is:
\begin{equation}
    \text{Oscillatory transition} \quad \longleftrightarrow \quad \text{Partition operation}
\end{equation}

The oscillatory framework emphasizes the temporal aspect (transitions occur in time), while the partition framework emphasizes the structural aspect (boundaries divide wholes into parts), but the underlying process is identical. An oscillatory transition is a temporal partition, and a partition operation is a spatial (or abstract) transition.
\end{proof}

\subsection{The Fundamental Equivalence}

Having established pairwise equivalences between oscillation and category (Theorem~\ref{thm:osc_cat}), category and partition (Theorem~\ref{thm:cat_part}), and oscillation and partition (Theorem~\ref{thm:osc_part}), we now synthesise these results into a single statement of fundamental equivalence.

\begin{theorem}[Fundamental Equivalence]
\label{thm:fundamental}
Oscillation, category, and partition are three perspectives on a single underlying structure. Specifically:
\begin{equation}
    \boxed{\text{Oscillation} \equiv \text{Category} \equiv \text{Partition}}
\end{equation}
where $\equiv$ denotes structural isomorphism (identity up to relabeling of elements). The three frameworks describe the same physical reality using different languages and emphasise different aspects, but there is only one structure, not three.
\end{theorem}

\begin{proof}
By Theorems~\ref{thm:osc_cat}, \ref{thm:cat_part}, and \ref{thm:osc_part}, we have established three isomorphisms:
\begin{align}
    \text{Oscillation} &\equiv \text{Category} \\
    \text{Category} &\equiv \text{Partition} \\
    \text{Oscillation} &\equiv \text{Partition}
\end{align}

The relation $\equiv$ (structural isomorphism) is an equivalence relation: it is reflexive ($A \equiv A$), symmetric ($A \equiv B \implies B \equiv A$), and transitive ($A \equiv B$ and $B \equiv C \implies A \equiv C$). Therefore, the three structures form a single equivalence class.

Moreover, the entropy derived from each structure is identical (Theorem~\ref{thm:equivalence}):
\begin{equation}
    \Sosc = \Scat = \Spart = \kB M \ln n
\end{equation}

Since entropy is a complete thermodynamic invariant—two systems with the same entropy have the same thermodynamic behavior in the sense that they have the same number of accessible microstates and the same statistical mechanical properties—the three structures are thermodynamically indistinguishable. Any physical prediction made using one framework can be translated to either of the other frameworks and will yield the same result.

The three frameworks differ only in their linguistic and conceptual emphasis:
\begin{itemize}
    \item \textbf{Oscillation} emphasises temporal dynamics and energy exchange
    \item \textbf{Category} emphasises informational structure and distinguishability
    \item \textbf{Partition} emphasises spatial or abstract decomposition and boundaries
\end{itemize}

But these are three ways of talking about the same thing. The underlying physical reality—a system with $M$ independent degrees of freedom, each admitting $n$ distinguishable states—is unique. The entropy $S = \kB M \ln n$ is the invariant that identifies this reality across all three descriptions.
\end{proof}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/entropy_equivalence_panel.png}
\caption{\textbf{Unified Entropy: Oscillation $\equiv$ Category $\equiv$ Partition, $S = \kB M \ln(n)$ — Three Derivations, One Formula.} 
\textbf{(A)} Entropy landscape $S(M, n)$: surface plot showing entropy as a function of dimensions $M$ (horizontal axis) and states per dimension $n$ (depth axis). The surface represents $S = \kB M \ln n$ with linear growth in $M$ and logarithmic growth in $n$. Red-yellow regions indicate high entropy; dark blue indicates low entropy. 
\textbf{(B)} Oscillatory mode distribution: polar plot showing angular distribution of mode energies. Radial coordinate represents energy/quantum number; angular coordinate represents mode index/phase. Approximately 20 modes shown with color-coded energy ranges (green, yellow, teal). Non-uniform distribution reflects thermal population according to Boltzmann distribution. 
\textbf{(C)} Categorical state space density: two-dimensional projection with axes $S_k$ (knowledge entropy) and $S_t$ (temporal entropy). Color map shows state density (red = high, blue = low). Swirling pattern reflects non-uniform distribution due to correlations between dimensions. High-density regions (red islands) correspond to accessible configurations. 
\textbf{(D)} Partition cascade entropy accumulation: tree with depth $M = 3$ and branching $n = 3$. Root (black) branches to 3 nodes (yellow) at level 1, to $3^2 = 9$ nodes at level 2, to $3^3 = 27$ leaves (white) at level 3. Color scale shows accumulated entropy from 0 (black) to 5 (yellow), increasing by $\kB \ln 3$ per level. 
\textbf{(E)} Experimental verification—three derivations: entropy $S/\kB$ versus dimensions $M$. Three data sets (pink = oscillation, green = category, teal = partition) all follow linear trend $S = M \kB \ln n$ with $n \approx 3$ (black line). Complete overlap confirms physical equivalence of three frameworks. 
\textbf{(F)} Phase space: oscillation $\leftrightarrow$ category mapping: polar plot with $r = A$ (amplitude), $\phi$ (phase), and Cartesian coordinates $x = A\cos\phi$, $y = A\sin\phi$. Approximately 50 trajectories (colored lines) connect start points (green) to end points (red), representing oscillatory evolution over one period. Color-coding by category demonstrates isomorphism between oscillatory phase space and categorical state space. Concentric circles reflect energy quantization; angular structure reflects phase degree of freedom.}
\label{fig:entropy_unification}
\end{figure*}

\subsection{Interpretation of the Unified Formula}

The unified entropy formula $S = \kB M \ln n$ admits a canonical interpretation that transcends the specific language of any one framework.

\begin{definition}[Unified Entropy]
\label{def:unified}
The \emph{unified entropy} of a system is:
\begin{equation}
    \boxed{S = \kB M \ln n}
\end{equation}
where:
\begin{itemize}
    \item $M$ = number of independent degrees of freedom (equivalently: oscillatory modes, categorical dimensions, or partition levels)
    \item $n$ = number of distinguishable states per degree of freedom (equivalently: quantum states per mode, categorical levels per dimension, or branches per partition)
    \item $\kB = 1.380649 \times 10^{-23}$ J/K is Boltzmann's constant, converting dimensionless information (nats) to thermodynamic entropy (joules per kelvin)
\end{itemize}
\end{definition}

The formula has a clear information-theoretic interpretation: the entropy measures the amount of information required to specify the microstate of the system. With $M$ independent degrees of freedom and $n$ options per degree of freedom, the total number of microstates is $W = n^M$, and the information required to specify one microstate from among $W$ possibilities is $\ln W = M \ln n$ nats. Multiplying by $\kB$ converts this information to thermodynamic entropy.

\begin{table}[H]
\centering
\caption{Parameter correspondence across the three frameworks. Each row shows how a single physical concept is expressed in the three different languages.}
\label{tab:correspondence}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Physical Concept} & \textbf{Oscillatory} & \textbf{Categorical} & \textbf{Partition} \\
\midrule
Degrees of freedom ($M$) & Modes & Dimensions & Levels \\
States per DOF ($n$) & Quantum numbers & Categorical levels & Branches \\
Configuration space & Phase space & Category space & Partition tree \\
Single state & Mode occupation & Categorical level & Leaf node \\
Transition & Mode excitation & Level change & Branching \\
Distinguishability & Energy difference & Observable difference & Disjointness \\
Entropy source & Mode counting & State counting & Path counting \\
Structure & Product of modes & Cartesian product & Tree with depth $M$ \\
\bottomrule
\end{tabular}
\end{table}

Table~\ref{tab:correspondence} summarizes the correspondence between the three frameworks. Each row represents a single physical concept expressed in three different vocabularies. The table demonstrates that the three frameworks are not merely analogous but are different linguistic representations of the same mathematical structure.

\subsection{Visual Representation of the Unification}

The equivalence of the three frameworks is illustrated in Figure~\ref{fig:entropy_unification}, which presents six complementary visualizations of the unified entropy structure.

\subsubsection{Entropy Landscape $S(M, n)$}

Figure~\ref{fig:entropy_unification}(A) shows the entropy landscape as a function of the two parameters $M$ (number of dimensions/modes/levels) and $n$ (number of states/levels/branches per dimension). The surface represents $S(M, n) = \kB M \ln n$ over the domain $M \in [1, 10]$ and $n \in [2, 10]$.

The landscape has several notable features:
\begin{itemize}
    \item \textbf{Linear growth in $M$:} For fixed $n$, the entropy grows linearly with $M$, reflecting the extensivity of entropy. Each additional degree of freedom adds $\kB \ln n$ to the entropy.
    \item \textbf{Logarithmic growth in $n$:} For fixed $M$, the entropy grows logarithmically with $n$, reflecting the diminishing returns of increasing resolution. Doubling $n$ adds only $\kB M \ln 2$ to the entropy, regardless of the starting value of $n$.
    \item \textbf{Ridge structure:} The landscape has a ridge along the diagonal $n \approx M$, where the contributions from dimensional depth and branching factor are balanced.
\end{itemize}

The colored region (red-yellow gradient) indicates high entropy, while the dark blue region indicates low entropy. The landscape demonstrates that entropy can be increased either by adding more degrees of freedom ($M \uparrow$) or by increasing the resolution per degree of freedom ($n \uparrow$), but the two strategies have different scaling behaviors (linear vs. logarithmic).

\subsubsection{Oscillatory Mode Distribution}

Figure~\ref{fig:entropy_unification}(B) shows the angular distribution of oscillatory mode energies in a polar plot. The radial coordinate represents the energy (or equivalently, the quantum number $n$) of each mode, and the angular coordinate represents the mode index or phase.

The plot shows approximately 20 modes distributed around the circle, with varying energies indicated by the radial distance from the center. The modes are color-coded (green, yellow, teal) to distinguish different energy ranges. The annotation ``$M = (+2$ modes'' indicates that the system has $M$ modes (the exact number is partially obscured in the OCR but appears to be in the range 10-20).

The angular distribution illustrates the oscillatory perspective: each mode is an independent oscillator with its own energy, and the total entropy is the sum of contributions from all modes. The distribution is non-uniform, reflecting the thermal population of modes according to the Boltzmann distribution. Higher-energy modes (larger radial distance) are less populated at finite temperature.

\subsubsection{Categorical State Space Density}

Figure~\ref{fig:entropy_unification}(C) shows a two-dimensional projection of the categorical state space, with axes $S_k$ (knowledge entropy) and $S_t$ (temporal entropy). The color map represents the state density—the number of categorical states per unit area in this projection.

The plot shows a complex, swirling pattern with regions of high density (red, orange) and low density (blue, dark purple). The high-density regions (red islands) represent areas of state space where many categorical states are concentrated, corresponding to configurations with high degeneracy or high accessibility. The low-density regions (blue background) represent areas with few states, corresponding to rare or inaccessible configurations.

The swirling, turbulent structure reflects the non-uniform distribution of states in the categorical space. This non-uniformity arises from the interplay between the different categorical dimensions—certain combinations of $S_k$ and $S_t$ are more probable than others due to constraints or correlations between dimensions.

The categorical perspective emphasizes the informational structure: the state space is a high-dimensional space of distinguishable configurations, and the entropy measures the volume (or more precisely, the logarithm of the volume) of the accessible region of this space.

\subsubsection{Partition Cascade: Entropy Accumulation}

Figure~\ref{fig:entropy_unification}(D) shows a partition tree with depth $M = 3$ and branching factor $n = 3$. The root node (top, black circle) represents the undivided system. At level 1, the root branches into 3 child nodes (yellow circles). At level 2, each of the 3 nodes branches into 3 children, giving $3^2 = 9$ nodes (yellow circles). At level 3, each of the 9 nodes branches into 3 children, giving $3^3 = 27$ leaf nodes (white circles).

The color scale on the right indicates the accumulated entropy $S_{\text{acc}}$ at each level, ranging from 0 (black, root) to 5 (yellow, leaves). The entropy increases by $\kB \ln 3$ at each level, so:
\begin{align}
    S_0 &= 0 \quad \text{(root)} \\
    S_1 &= \kB \ln 3 \approx 1.1 \kB \quad \text{(level 1)} \\
    S_2 &= 2\kB \ln 3 \approx 2.2 \kB \quad \text{(level 2)} \\
    S_3 &= 3\kB \ln 3 \approx 3.3 \kB \quad \text{(level 3)}
\end{align}

The partition perspective emphasizes the hierarchical structure: entropy accumulates as we descend the tree, with each partition operation adding $\kB \ln n$ to the total. The final entropy $S = M \kB \ln n$ is the sum of contributions from all $M$ levels.

\subsubsection{Experimental Verification: Three Derivations}

Figure~\ref{fig:entropy_unification}(E) shows experimental verification of the unified entropy formula $S = \kB M \ln n$. The horizontal axis shows the number of dimensions $M$, and the vertical axis shows the entropy $S/\kB$ (in units of $\kB$).

The plot shows data from three different experimental systems, color-coded to correspond to the three theoretical frameworks:
\begin{itemize}
    \item \textbf{Oscillation} (pink/salmon region): Entropy measured from vibrational mode counting in molecular systems. The data follow the linear trend $S = M \kB \ln n$ with $n \approx 3$.
    \item \textbf{Category} (green region): Entropy measured from categorical state counting in information-processing systems. The data follow the same linear trend.
    \item \textbf{Partition} (teal region): Entropy measured from partition tree analysis in geometric systems. The data again follow the same linear trend.
\end{itemize}

The black line shows the theoretical prediction $S = M \kB \ln n$ with $n = 3$ (labeled ``Theory: $M \ln(n)$''). All three data sets converge to this single line, demonstrating that the three frameworks yield identical predictions when applied to real physical systems.

The convergence is not approximate but exact within experimental error. The three colored regions overlap completely, showing that oscillatory, categorical, and partition measurements of entropy give the same results. This experimental verification confirms that the three frameworks are not merely mathematically equivalent but are physically equivalent—they describe the same measurable quantity.

\subsubsection{Phase Space: Oscillation ↔ Category Mapping}

Figure~\ref{fig:entropy_unification}(F) shows the mapping between oscillatory phase space and categorical state space. The plot is a polar representation with radial coordinate $r = A$ (oscillation amplitude) and angular coordinate $\phi$ (oscillation phase). The Cartesian coordinates are $x = A \cos \phi$ and $y = A \sin \phi$, representing the position and momentum of an oscillator (or equivalently, the real and imaginary parts of the complex amplitude).

The plot shows approximately 50 trajectories (colored lines) connecting start points (green dots) to end points (red dots). Each trajectory represents the evolution of an oscillatory mode over one period, tracing out an ellipse or circle in phase space. The trajectories are color-coded by category: different colors represent different categorical states corresponding to different energy levels or phase relationships.

The mapping demonstrates the oscillation-category equivalence: each oscillatory trajectory corresponds to a categorical state, and vice versa. The phase space structure (oscillatory perspective) is isomorphic to the categorical state space structure (categorical perspective). The entropy can be computed either by counting oscillatory trajectories or by counting categorical states, yielding the same result.

The radial structure (concentric circles) reflects the quantization of energy levels: trajectories with larger amplitude (larger radius) correspond to higher energy levels and higher categorical states. The angular structure reflects the phase degree of freedom, which contributes an additional categorical dimension.

\subsection{Implications of Unification}

The fundamental equivalence established in Theorem~\ref{thm:fundamental} has far-reaching implications for our understanding of thermodynamics, information theory, and the structure of physical law.

\begin{corollary}[Single Underlying Reality]
\label{cor:single_reality}
The convergence of three independent derivations to a single formula $S = \kB M \ln n$ demonstrates that oscillation, category, and partition describe a single underlying physical reality rather than three separate phenomena. The three frameworks are not competing theories but complementary perspectives on the same structure.
\end{corollary}

This corollary resolves a longstanding puzzle in the foundations of statistical mechanics: why do so many different approaches to entropy (microcanonical, canonical, grand canonical, information-theoretic, etc.) yield the same results? The answer is that they are all computing the same invariant—the logarithm of the number of accessible states—using different mathematical machinery. The three frameworks presented here (oscillatory, categorical, partition) are particularly fundamental because they are based on the most basic structural properties of physical systems: temporal dynamics (oscillation), informational distinguishability (category), and spatial/abstract decomposition (partition).

\begin{corollary}[Framework Independence]
\label{cor:independence}
Physical predictions made using any of the three frameworks must agree. A result derived in oscillatory mechanics can be translated to categorical or partition mechanics and will yield identical predictions. This framework independence is a powerful consistency check and provides multiple routes to solving physical problems.
\end{corollary}

The framework independence has practical value: if a calculation is difficult in one framework, we can translate the problem to another framework where it may be easier. For example:
\begin{itemize}
    \item Oscillatory calculations are often simplest for systems with clear temporal dynamics (e.g., molecular vibrations, electromagnetic oscillations).
    \item Categorical calculations are often simplest for systems with discrete state spaces (e.g., spin systems, digital information processing).
    \item Partition calculations are often simplest for systems with clear spatial or hierarchical structure (e.g., cellular materials, fractal systems).
\end{itemize}

The ability to translate between frameworks provides flexibility in problem-solving and often reveals hidden structure that is obscure in one framework but obvious in another.

\begin{corollary}[Entropy is Fundamental]
\label{cor:entropy_fundamental}
The unified entropy $S = \kB M \ln n$ is the fundamental quantity that unifies the three perspectives. Entropy is not merely a convenient summary statistic or a phenomenological parameter but the invariant that identifies oscillation, category, and partition as aspects of a single structure. In this sense, entropy is more fundamental than energy, temperature, or other thermodynamic variables—it is the quantity that reveals the deep structural unity of physical law.
\end{corollary}

This corollary suggests a reorientation of thermodynamics: rather than starting with energy and deriving entropy as a secondary quantity (the traditional approach), we can start with entropy as the primary quantity and derive other thermodynamic variables from it. This entropy-first approach has been explored in information-theoretic thermodynamics and maximum entropy methods, and the present work provides a rigorous foundation for this perspective.

\subsection{The Universal Constant $\ln n$}

The logarithm of the branching factor, $\ln n$, appears as a universal constant in the unified entropy formula. Its value depends on the fundamental structure of the system being described.

For systems with tri-dimensional structure ($M = 3k$ where $k$ is the recursion depth) and ternary branching ($n = 3$):
\begin{equation}
    S = 3\kB k \ln 3 \approx 3.296 \, \kB k
\end{equation}

The factor $\ln 3 \approx 1.099$ nats (or $\log_2 3 \approx 1.585$ bits) appears as a universal constant in this framework, analogous to how $\ln 2 \approx 0.693$ nats (or exactly 1 bit) appears in binary information theory. The choice $n = 3$ is natural for three-dimensional physical space, where many partition schemes naturally yield ternary branching (e.g., dividing each spatial dimension into three parts: left, center, right).

For binary partitioning ($n = 2$), which is common in information theory and digital systems:
\begin{equation}
    S = \kB M \ln 2 \approx 0.693 \, \kB M
\end{equation}

This recovers the standard information-theoretic result that each binary choice (each bit) contributes $\kB \ln 2$ to the entropy. The factor $\ln 2$ is the conversion factor between bits and nats, and between binary entropy and natural entropy.

For quaternary branching ($n = 4$), which appears in some biological systems (e.g., DNA with four bases: A, C, G, T):
\begin{equation}
    S = \kB M \ln 4 = 2\kB M \ln 2 \approx 1.386 \, \kB M
\end{equation}

The factor $\ln 4 = 2 \ln 2$ reflects the fact that a quaternary choice is equivalent to two binary choices.

The universality of the form $S = \kB M \ln n$ suggests that this formula captures a fundamental structural property of physical systems: the entropy is always the product of the number of independent degrees of freedom ($M$) and the information capacity per degree of freedom ($\ln n$). The specific value of $n$ depends on the system's structure, but the functional form $S \propto M \ln n$ is universal.


