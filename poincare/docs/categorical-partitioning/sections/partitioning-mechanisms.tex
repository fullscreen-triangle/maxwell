\section{Entropy from Partition Mechanics}
\label{sec:partition}

We derive entropy from first principles of partition operations, making no reference to oscillatory dynamics or categorical structure. The derivation rests solely on the combinatorics of dividing systems into distinguishable parts. This independent derivation establishes the partition perspective as the third of three equivalent foundations for thermodynamic entropy, completing the triad of oscillation, categorization, and partition.

\subsection{Axioms of Partition Operations}

The concept of partitioning formalizes the intuitive notion that complex systems can be understood by dividing them into simpler subsystems. We begin by establishing the axioms that define partition structure, proceeding from the existence of partitions to their recursive properties.

\begin{axiom}[Partition Existence]
\label{axiom:partition_exist}
Any system $X$ with structure can be partitioned into subsystems. A \emph{partition} of $X$ is a collection $\mathcal{P} = \{X_1, X_2, \ldots, X_n\}$ such that:
\begin{enumerate}[(i)]
    \item \textbf{Disjointness:} $X_i \cap X_j = \emptyset$ for all $i \neq j$ (the subsystems do not overlap)
    \item \textbf{Exhaustiveness:} $\bigcup_{i=1}^{n} X_i = X$ (the subsystems cover the entire system)
    \item \textbf{Non-triviality:} Each $X_i$ is non-empty and contains at least one distinguishable element
\end{enumerate}
\end{axiom}

The partition existence axiom asserts that structured systems admit decomposition into disjoint, exhaustive subsystems. This is the mathematical formalization of the physical intuition that any extended object can be divided into parts. The disjointness condition ensures that each element of the system belongs to exactly one subsystem—there is no ambiguity about which part an element belongs to. The exhaustiveness condition ensures that no elements are left out—the partition accounts for the entire system. The non-triviality condition excludes degenerate cases where a ``subsystem'' is empty or contains no distinguishable structure.

The axiom applies to systems with any kind of structure: spatial structure (dividing a region of space into subregions), temporal structure (dividing a time interval into subintervals), energy structure (dividing an energy range into energy bins), or abstract structure (dividing a set into subsets). The key requirement is that the system has enough structure to support meaningful distinctions between parts.

\begin{axiom}[Branching Factor]
\label{axiom:branching}
Each partition operation divides a system into $n$ subsystems, where $n \geq 2$ is the \emph{branching factor}. The branching factor is determined by the structure of the system being partitioned and remains constant across all partition operations on systems of the same type.
\end{axiom}

The branching factor axiom specifies how many parts result from each partition operation. The constraint $n \geq 2$ ensures that partitioning is non-trivial—dividing a system into a single part is not a partition but the identity operation. The assumption of constant branching factor reflects the self-similar nature of partition operations: if a system has a natural way of being divided into $n$ parts, then each of those parts, being structurally similar to the original system, also divides naturally into $n$ parts.

The branching factor is not arbitrary but is determined by the dimensionality and symmetry of the system. For spatial systems in $d$ dimensions, natural branching factors include:
\begin{itemize}
    \item \textbf{Binary partition per dimension:} $n = 2$, dividing each dimension into two halves. For $d$ dimensions, this yields $2^d$ total parts after partitioning all dimensions simultaneously.
    \item \textbf{Simplicial partition:} $n = d + 1$, dividing space into simplices (triangles in 2D, tetrahedra in 3D).
    \item \textbf{Orthogonal partition:} $n = 2d$, dividing space along the positive and negative directions of each coordinate axis.
\end{itemize}

For three-dimensional physical space ($d = 3$), common branching factors are $n = 2$ (binary), $n = 3$ (ternary), $n = 4$ (tetrahedral), or $n = 8$ (octahedral). The choice depends on the symmetry of the system and the physical process implementing the partition.

\begin{axiom}[Recursive Partitionability]
\label{axiom:recursive_part}
Each subsystem $X_i$ produced by a partition is itself partitionable, admitting the same partition structure as the parent system. Partitioning can be applied recursively to arbitrary depth, subject only to physical constraints on the minimum size or resolution of subsystems.
\end{axiom}

The recursive partitionability axiom asserts that partition structure is scale-invariant—the same pattern of division repeats at all scales. This is the partition analog of the recursive decomposition axiom for categorical spaces (Axiom~\ref{axiom:recursive}). The axiom reflects the self-similar nature of physical systems: if a region of space can be divided into subregions, then each subregion, being itself a region of space, can be further divided using the same procedure.

The recursion terminates at finite depth due to physical constraints. In quantum mechanics, the Heisenberg uncertainty principle prevents subdivision below the Planck scale. In thermodynamics, the atomic structure of matter prevents subdivision below the molecular scale. In information theory, the finite precision of measurements prevents infinite resolution. These physical constraints ensure that the partition tree has finite depth, yielding a finite number of distinguishable states and finite entropy.

\begin{definition}[Partition Tree]
\label{def:partition_tree}
A \emph{partition tree} of depth $k$ is the hierarchical structure produced by applying $k$ successive partition operations to an initial system $X^{(0)}$. The tree has the following properties:
\begin{itemize}
    \item The \textbf{root} (level 0) represents the undivided system $X^{(0)}$
    \item Each \textbf{node} at level $j \in \{1, 2, \ldots, k-1\}$ represents a subsystem produced by $j$ partition operations
    \item Each node at level $j < k$ has exactly $n$ \textbf{children} at level $j+1$, corresponding to the $n$ subsystems produced by partitioning
    \item The \textbf{leaves} (level $k$) represent the terminal subsystems produced by $k$ partition operations
\end{itemize}
The partition tree encodes the complete history of partition operations and the hierarchical structure of the resulting subsystems.
\end{definition}

The partition tree is a mathematical representation of the recursive partition structure. It is analogous to the decision tree in information theory, the phylogenetic tree in evolutionary biology, or the Feynman path integral in quantum mechanics—each represents a branching process where a single initial state gives rise to multiple descendant states through successive operations.

\subsection{Combinatorics of Partition Trees}

Having established the axioms defining partition operations, we now derive the fundamental combinatorial properties of partition trees. These properties determine the number of distinguishable states produced by recursive partitioning and form the basis for the entropy calculation.

\begin{theorem}[Number of Partition Paths]
\label{thm:partition_paths}
For a partition tree of depth $k$ with branching factor $n$, the number of distinct paths from root to leaf is:
\begin{equation}
    P(k, n) = n^k
\end{equation}
where a path is a sequence of partition choices specifying which branch to follow at each level.
\end{theorem}

\begin{proof}
A path from root to leaf is determined by making a choice at each of the $k$ levels of the partition tree. At each level $j \in \{1, 2, \ldots, k\}$, there are $n$ possible branches to follow, corresponding to the $n$ subsystems produced by the partition operation at that level.

Since the choices at different levels are independent (the partition structure is the same at all levels by Axiom~\ref{axiom:recursive_part}), the total number of distinct paths is the product of the number of choices at each level:
\begin{equation}
    P(k, n) = \underbrace{n \times n \times \cdots \times n}_{k \text{ factors}} = n^k
\end{equation}

This is the fundamental counting principle: when making $k$ independent choices with $n$ options each, the total number of possible outcomes is $n^k$.
\end{proof}

\begin{theorem}[Number of Leaf Nodes]
\label{thm:leaf_nodes}
A partition tree of depth $k$ with branching factor $n$ has exactly $n^k$ leaf nodes (terminal subsystems at level $k$).
\end{theorem}

\begin{proof}
We proceed by induction on the depth $k$.

\textbf{Base case} ($k = 0$): At depth $k = 0$, the tree consists of only the root node, which is itself a leaf (since there are no children). The number of leaves is $1 = n^0$, so the base case holds.

\textbf{Base case} ($k = 1$): At depth $k = 1$, the root node is partitioned into $n$ children, all of which are leaves. The number of leaves is $n = n^1$, so the base case holds.

\textbf{Inductive step}: Assume the theorem holds at depth $k$, so there are $n^k$ leaf nodes at level $k$. At depth $k+1$, each of the $n^k$ nodes at level $k$ is partitioned into $n$ children. The total number of nodes at level $k+1$ is:
\begin{equation}
    n^k \times n = n^{k+1}
\end{equation}

All of these nodes are leaves (since we stop partitioning at level $k+1$). By induction, the theorem holds for all $k \geq 0$.
\end{proof}

\begin{corollary}[Total Number of Nodes]
\label{cor:total_nodes}
A partition tree of depth $k$ with branching factor $n$ has a total of:
\begin{equation}
    N_{\text{total}} = \sum_{j=0}^{k} n^j = \frac{n^{k+1} - 1}{n - 1}
\end{equation}
nodes across all levels.
\end{corollary}

\begin{proof}
At level $j$, there are $n^j$ nodes (by induction or direct counting). Summing over all levels from $0$ to $k$:
\begin{equation}
    N_{\text{total}} = \sum_{j=0}^{k} n^j
\end{equation}

This is a geometric series with first term $a = 1$, common ratio $r = n$, and $k+1$ terms. The sum is:
\begin{equation}
    N_{\text{total}} = \frac{n^{k+1} - 1}{n - 1}
\end{equation}

For large $k$, this is dominated by the leaf nodes: $N_{\text{total}} \approx n^k/(n-1) \approx n^k$ for $n \gg 1$.
\end{proof}

\subsection{Partition Entropy from Branching Structure}

We now derive the entropy associated with partition trees by applying information-theoretic principles to the combinatorics of partition paths. The key insight is that entropy measures the uncertainty about which leaf node is occupied when traversing the tree from root to leaf.

\begin{definition}[Partition Entropy]
\label{def:partition_entropy}
The \emph{partition entropy} of a partition tree measures the uncertainty about which leaf node (terminal partition element) is occupied when traversing the tree from root to leaf. If all leaf nodes are equally probable, the partition entropy is:
\begin{equation}
    \Spart = \kB \ln(\text{number of leaf nodes})
\end{equation}
This is the Boltzmann entropy applied to the partition tree structure.
\end{definition}

The partition entropy quantifies the amount of information required to specify which particular leaf node (terminal subsystem) is occupied. Equivalently, it measures the amount of information gained by observing which path through the tree is realized. The entropy is maximized when all leaf nodes are equally probable, corresponding to the microcanonical ensemble in statistical mechanics.

\begin{theorem}[Partition Entropy per Level]
\label{thm:entropy_per_level}
Each partition operation (each level of the tree) contributes an entropy increment of:
\begin{equation}
    \Delta S_{\text{level}} = \kB \ln n
\end{equation}
where $n$ is the branching factor.
\end{theorem}

\begin{proof}
At each partition operation, a single parent system is divided into $n$ child systems. If we have no information to prefer one child over another, we assign equal probability to each child:
\begin{equation}
    p_i = \frac{1}{n} \quad \text{for } i = 1, 2, \ldots, n
\end{equation}

The Shannon entropy of this uniform distribution over $n$ outcomes is:
\begin{equation}
    H = -\sum_{i=1}^{n} p_i \ln p_i = -\sum_{i=1}^{n} \frac{1}{n} \ln \frac{1}{n} = -n \cdot \frac{1}{n} \ln \frac{1}{n} = -\ln \frac{1}{n} = \ln n
\end{equation}

This is the information-theoretic entropy measured in nats (natural units). Converting to thermodynamic entropy by multiplying by Boltzmann's constant:
\begin{equation}
    \Delta S_{\text{level}} = \kB H = \kB \ln n
\end{equation}

This result has a clear interpretation: each partition operation generates $\ln n$ nats of information (or $\log_2 n$ bits), corresponding to the uncertainty about which of the $n$ branches is realized. The entropy increment is independent of the level—each partition contributes the same amount of entropy, reflecting the self-similar structure of the partition tree.
\end{proof}

\begin{theorem}[Total Partition Entropy]
\label{thm:partition_entropy}
For a partition tree of depth $M$ with branching factor $n$, the total entropy is:
\begin{equation}
    \boxed{\Spart = \kB M \ln n}
\end{equation}
where $M$ is the number of partition levels (depth of recursive partitioning).
\end{theorem}

\begin{proof}
We present two proofs, one based on additive entropy contributions and one based on direct state counting.

\textbf{Proof 1 (Additive):} The total entropy is the sum of entropy contributions from each level. With $M$ levels, each contributing $\Delta S_{\text{level}} = \kB \ln n$ by Theorem~\ref{thm:entropy_per_level}:
\begin{equation}
    \Spart = \sum_{j=1}^{M} \Delta S_{\text{level}} = \sum_{j=1}^{M} \kB \ln n = M \kB \ln n
\end{equation}

This additive structure reflects the fact that entropy is extensive—each additional partition level adds an independent contribution to the total entropy.

\textbf{Proof 2 (Multiplicative):} The number of distinguishable leaf nodes is $n^M$ by Theorem~\ref{thm:leaf_nodes}. Applying the Boltzmann relation $S = \kB \ln W$ with $W = n^M$ the number of accessible microstates:
\begin{equation}
    \Spart = \kB \ln(n^M) = \kB M \ln n
\end{equation}

The two proofs are equivalent, reflecting the fundamental connection between additive entropy (sum over levels) and multiplicative state counting (product over levels). The logarithm converts the multiplicative structure $W = n^M$ into the additive structure $S = M \ln n$.
\end{proof}

\begin{remark}[Physical Interpretation]
The entropy $\Spart = \kB M \ln n$ has the following interpretation in the partition framework:

\begin{itemize}
    \item \textbf{Partition depth $M$:} The parameter $M$ counts the number of partition levels—the depth of recursive subdivision applied to the system. This is the partition analog of the number of dimensions in categorical space or the number of oscillatory modes in oscillatory mechanics. The linear dependence $S \propto M$ reflects the extensivity of entropy: each additional partition level multiplies the number of states by $n$, adding $\ln n$ to the entropy.
    
    \item \textbf{Branching factor $n$:} The parameter $n$ counts the number of parts produced by each partition operation. This is the partition analog of the number of categorical levels per dimension or the number of quantum states per oscillatory mode. The logarithmic dependence $S \propto \ln n$ reflects the information content: dividing a system into $n$ parts generates $\ln n$ nats of information about which part is occupied.
    
    \item \textbf{Information per partition:} The quantity $\ln n$ represents the information generated per partition operation, measured in nats. For binary partitions ($n = 2$), this is $\ln 2 \approx 0.693$ nats or exactly 1 bit. For ternary partitions ($n = 3$), this is $\ln 3 \approx 1.099$ nats or approximately 1.585 bits. For octahedral partitions ($n = 8$), this is $\ln 8 = 3 \ln 2$ nats or exactly 3 bits.
    
    \item \textbf{Boltzmann's constant:} The factor $\kB = 1.380649 \times 10^{-23}$ J/K converts from dimensionless information (nats) to thermodynamic entropy (joules per kelvin). This conversion establishes the connection between the abstract partition structure and the physical thermodynamic quantity.
\end{itemize}

The formula $\Spart = \kB M \ln n$ is structurally identical to the oscillatory entropy $\Sosc = \kB M \ln n$ and the categorical entropy $\Scat = \kB M \ln n$, despite the three derivations proceeding from entirely different axioms. This structural identity reveals a fundamental equivalence between oscillation, categorization, and partition—three perspectives on a single underlying structure.
\end{remark}

\subsection{Dimensional Interpretation of Partition Depth}

The partition depth $M$ has a natural interpretation in terms of the dimensionality of the space being partitioned. For systems embedded in $d$-dimensional space, the branching factor and partition depth are related to the spatial dimension.

\begin{theorem}[Partition Depth as Dimensionality]
\label{thm:depth_dimension}
For systems embedded in $d$-dimensional space, natural partition schemes yield branching factors:
\begin{itemize}
    \item \textbf{Binary per dimension:} $n = 2$ with $M = d \cdot k$ levels, where $k$ is the recursion depth. Each dimension is subdivided $k$ times, yielding $2^{dk}$ total subsystems.
    \item \textbf{Hyperoctant partition:} $n = 2^d$ with $M = k$ levels, dividing space into $2^d$ orthants at each level, yielding $(2^d)^k = 2^{dk}$ total subsystems.
    \item \textbf{Simplicial partition:} $n = d + 1$ with $M = k$ levels, dividing space into $(d+1)$-simplices, yielding $(d+1)^k$ total subsystems.
\end{itemize}
\end{theorem}

For three-dimensional physical space ($d = 3$), common partition schemes include:

\begin{itemize}
    \item \textbf{Binary per dimension:} $n = 2$, $M = 3k$, yielding $2^{3k} = 8^k$ subsystems and entropy:
    \begin{equation}
        \Spart = \kB \cdot 3k \cdot \ln 2 = 3k \kB \ln 2
    \end{equation}
    
    \item \textbf{Ternary per dimension:} $n = 3$, $M = 3k$, yielding $3^{3k} = 27^k$ subsystems and entropy:
    \begin{equation}
        \Spart = \kB \cdot 3k \cdot \ln 3 = 3k \kB \ln 3
    \end{equation}
    
    \item \textbf{Octahedral partition:} $n = 8$, $M = k$, yielding $8^k$ subsystems and entropy:
    \begin{equation}
        \Spart = \kB \cdot k \cdot \ln 8 = 3k \kB \ln 2
    \end{equation}
    (equivalent to binary per dimension)
    
    \item \textbf{Tetrahedral partition:} $n = 4$, $M = k$, yielding $4^k$ subsystems and entropy:
    \begin{equation}
        \Spart = \kB \cdot k \cdot \ln 4 = 2k \kB \ln 2
    \end{equation}
\end{itemize}

The ternary partition with $M = 3k$ is particularly natural for three-dimensional space, as it treats all three dimensions symmetrically and yields the $27^k$ growth observed in the categorical derivation (Section~\ref{sec:categorical}, Corollary~\ref{cor:exponential_growth}).

\subsection{Sequential Partition and History Dependence}

Partition operations are inherently sequential—each partition creates new boundaries that persist through subsequent partitions. This sequential structure gives rise to a notion of partition history, which encodes the complete sequence of partition operations and determines the final state of the system.

\begin{definition}[Partition History]
\label{def:partition_history}
The \emph{partition history} $H_k$ of a system at depth $k$ is the sequence of partition choices made to reach the current state:
\begin{equation}
    H_k = (h_1, h_2, \ldots, h_k)
\end{equation}
where $h_j \in \{1, 2, \ldots, n\}$ specifies which branch was taken at level $j$ (which of the $n$ subsystems was selected at partition $j$).
\end{definition}

The partition history is a path through the partition tree from root to leaf. It specifies not only the final state (which leaf node is occupied) but also the sequence of intermediate states (which nodes were visited along the path). Two systems with the same final state but different partition histories are distinguishable if the history is observable.

\begin{theorem}[History Encodes Entropy]
\label{thm:history_entropy}
The partition history $H_k$ encodes exactly $\Spart = \kB k \ln n$ nats of information. Systems with identical current states but different histories are entropically distinguishable if the history is observable.
\end{theorem}

\begin{proof}
The partition history is a sequence of $k$ symbols, each drawn from an alphabet of size $n$. The number of distinct histories is $n^k$ by Theorem~\ref{thm:partition_paths}. If all histories are equiprobable (uniform distribution over all paths), the information content is:
\begin{equation}
    I = \ln(\text{number of histories}) = \ln(n^k) = k \ln n \text{ nats}
\end{equation}

Converting to thermodynamic entropy:
\begin{equation}
    \Spart = \kB I = \kB k \ln n
\end{equation}

The entropy of a system is determined by its full partition history, not merely its current configuration. Two systems in identical final configurations but with different partition histories have different entropies if the history is observable (e.g., if the boundaries created by intermediate partitions persist in the final state).

This history dependence is the partition analog of path dependence in thermodynamics: the entropy of a system depends on the process by which it reached its current state, not only on the current state itself. This is consistent with the second law of thermodynamics, which states that entropy is a state function for reversible processes but increases for irreversible processes.
\end{proof}

\begin{remark}[Observable vs. Hidden History]
The observability of partition history depends on whether the boundaries created by partition operations persist or are erased. In many physical systems, boundaries persist (e.g., cell division in biology, crack propagation in materials, phase separation in mixtures), making the history observable and contributing to the entropy. In other systems, boundaries are erased by diffusion or mixing, making the history unobservable and reducing the effective entropy to that of the final configuration alone.

The distinction between observable and hidden history is analogous to the distinction between fine-grained and coarse-grained entropy in statistical mechanics. The fine-grained entropy includes all microscopic details (including history), while the coarse-grained entropy averages over unobservable details. The partition framework naturally accommodates both perspectives by allowing us to choose whether to include history in the state specification.
\end{remark}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/geometric_partitioning_panel.png}
\caption{\textbf{Geometric Partitioning: Virtual Aperture Experiments (REAL DATA).} 
\textbf{(A)} Temperature independence [REAL EXPERIMENT]: selectivity versus temperature from 100 K to 600 K. The selectivity remains constant at $s \approx 0.096$ (red dashed line, shaded region shows variance $\sigma^2 = 0.008266$), demonstrating that apertures select by configuration, not velocity. This temperature independence is a signature of geometric partitioning—the aperture creates a boundary in configuration space that persists regardless of kinetic energy. 
\textbf{(B)} Categorical exclusion [REAL EXPERIMENT]: enhancement factor (ratio of transmitted to blocked molecules) versus membrane potential. Red data points show exponential decay following $\exp(q \Delta \phi / kT)$ (gray dashed line), where the aperture creates an energy barrier and only molecules with sufficient energy can pass through. The exponential dependence reflects the Boltzmann distribution. 
\textbf{(C)} Cascade selectivity amplification [REAL EXPERIMENT]: total selectivity (log scale) versus cascade length (number of apertures in series). Green circles show measured selectivity decreasing exponentially as $s^n$ with $s = 0.5$ per aperture (purple dashed line). Multiple partition operations amplify selectivity multiplicatively, demonstrating the exponential structure of sequential partitions. 
\textbf{(D)} Aperture in $S$-space [REAL MOLECULE DISTRIBUTION]: distribution of approximately 200 molecules in two-dimensional $S$-space with axes $S_k$ (knowledge entropy) and $S_t$ (temporal entropy). Red dots represent blocked molecules; green dots represent transmitted molecules. Blue dashed circle indicates aperture boundary with selectivity $19.60\%$. This demonstrates that partition operations can be defined in abstract entropy spaces as well as physical position space. 
\textbf{(E)} Charge field creates aperture [REAL EXPERIMENT]: selectivity versus membrane potential. Horizontal bars show selectivity at four potentials: $-30$ mV ($s = 0.61$, purple), $-50$ mV ($s = 0.23$, teal), $-70$ mV ($s = 0.03$, green), $-100$ mV ($s \approx 0.00$, white). Selectivity decreases exponentially with potential magnitude, consistent with Boltzmann factor. Electric field geometry defines aperture shape—the potential creates a geometric boundary in phase space. 
\textbf{(F)} Zero information selection [REAL DEMONSTRATION]: histogram of selectivity per trial across multiple experimental runs. Distribution is approximately uniform with mean $\bar{s} = 0.277$ (red line). Green annotation emphasizes: no probability distribution updated, no wavefunction collapse, no Landauer erasure. Geometric partition operations do not require information processing—they passively divide phase space based on geometric criteria without acquiring or erasing information about individual molecules.}
\label{fig:partition_experiments}
\end{figure*}

\subsection{Physical Realizations of Partition Mechanics}

The abstract partition framework has concrete physical realizations in various domains. We now examine several experimental systems that exhibit partition-like behavior, demonstrating that partition entropy is not merely a mathematical abstraction but a measurable physical quantity.

\subsubsection{Geometric Partitioning and Virtual Apertures}

One physical realization of partition mechanics is the concept of \emph{virtual apertures}—regions of phase space that selectively transmit certain states while blocking others. This is illustrated in Figure~\ref{fig:partition_experiments}, which presents experimental data from molecular selectivity experiments.

\begin{theorem}[Temperature Independence of Geometric Selection]
\label{thm:temp_independence}
Geometric partition operations (virtual apertures) select states based on configuration, not on velocity or temperature. The selectivity $s$ is independent of temperature $T$ over a wide range.
\end{theorem}

This is demonstrated in Figure~\ref{fig:partition_experiments}(A), which shows experimental measurements of selectivity versus temperature from 100 K to 600 K. The selectivity remains constant at $s \approx 0.096$ (red dashed line) with variance $\sigma^2 = 0.008266$, despite the temperature varying by a factor of 6. The blue annotation emphasizes the key result: ``Apertures select by configuration, not velocity.''

This temperature independence is a signature of geometric partitioning—the aperture creates a boundary in configuration space that persists regardless of the kinetic energy of the particles. This is in contrast to energy-based selection (e.g., Maxwell's demon), which would show strong temperature dependence. The geometric nature of the partition is the physical realization of the abstract partition axioms: the system is divided into ``passes through aperture'' and ``blocked by aperture'' subsystems based purely on spatial configuration.

\begin{theorem}[Categorical Exclusion and Exponential Enhancement]
\label{thm:categorical_exclusion}
Virtual apertures create categorical boundaries in phase space, leading to exponential enhancement of selectivity with the potential difference across the aperture.
\end{theorem}

This is demonstrated in Figure~\ref{fig:partition_experiments}(B), which shows the enhancement factor (ratio of transmitted to blocked molecules) as a function of membrane potential. The red data points follow an exponential decay:
\begin{equation}
    \text{Enhancement} \propto \exp\left(\frac{q \Delta \phi}{kT}\right)
\end{equation}
where $q$ is the charge, $\Delta \phi$ is the potential difference, $k$ is Boltzmann's constant, and $T$ is temperature. The gray dashed line shows the theoretical prediction $\exp(q \Delta \phi / kT)$ for concentration enhancement.

The exponential dependence reflects the Boltzmann distribution: the probability of occupying a state with energy $E$ is $\propto \exp(-E/kT)$. The aperture creates an energy barrier $\Delta E = q \Delta \phi$, and only molecules with sufficient energy to overcome this barrier can pass through. The enhancement factor is the ratio of probabilities on the two sides of the barrier, yielding the exponential form.

\begin{theorem}[Cascade Selectivity Amplification]
\label{thm:cascade_amplification}
Multiple partition operations in series (cascade) amplify selectivity exponentially. For $n$ apertures in series, each with selectivity $s$, the total selectivity is:
\begin{equation}
    S_{\text{total}} = s^n
\end{equation}
\end{theorem}

This is demonstrated in Figure~\ref{fig:partition_experiments}(C), which shows total selectivity (vertical axis, log scale) versus cascade length (horizontal axis, number of apertures). The green data points (circles) show measured selectivity, which decreases exponentially with cascade length. The purple dashed line shows the theoretical prediction $S_{\text{total}} = s^n$ with $s = 0.5$ per aperture.

The exponential amplification is a direct consequence of the multiplicative structure of sequential partitions. If each aperture transmits a fraction $s$ of the incident molecules, then $n$ apertures in series transmit a fraction $s \times s \times \cdots \times s = s^n$. This is the partition analog of the exponential growth of states with partition depth: just as the number of states grows as $n^M$, the selectivity (fraction of states transmitted) decreases as $s^M$ for $s < 1$.

\subsubsection{Apertures in $S$-Space and Molecular Distribution}

The partition framework extends naturally to abstract state spaces beyond physical configuration space. Figure~\ref{fig:partition_experiments}(D) shows the distribution of molecules in a two-dimensional projection of $S$-space, with axes $S_k$ (knowledge entropy) and $S_t$ (temporal entropy).

The plot shows approximately 200 molecules (dots) distributed across the $S$-space, with red dots representing molecules blocked by the aperture and green dots representing molecules that passed through. The blue dashed circle indicates the aperture boundary—a geometric region in $S$-space that selectively transmits molecules based on their location in this abstract space.

The selectivity is $19.60\%$ (indicated in the legend), meaning that approximately 1 in 5 molecules passes through the aperture. This selectivity is determined purely by the geometry of the aperture in $S$-space, not by any dynamical or energetic considerations. The molecules are distributed according to some underlying probability distribution (e.g., thermal equilibrium), and the aperture simply partitions this distribution into ``inside'' and ``outside'' regions.

This demonstrates that partition operations can be defined in abstract spaces (entropy spaces, phase spaces, configuration spaces) as well as in physical position space. The mathematical structure is the same: a boundary divides the space into disjoint regions, and the entropy is determined by counting the number of distinguishable regions.

\subsubsection{Charge Field Creates Aperture}

Figure~\ref{fig:partition_experiments}(E) shows how an electric field creates a virtual aperture by modifying the energy landscape. The horizontal axis shows selectivity (fraction of molecules transmitted), and the vertical axis shows membrane potential (voltage across the aperture).

Three potential values are shown:
\begin{itemize}
    \item $-30$ mV (purple bar): high selectivity $s = 0.61$
    \item $-50$ mV (teal bar): moderate selectivity $s = 0.23$
    \item $-70$ mV (green bar): low selectivity $s = 0.03$
    \item $-100$ mV (white bar): negligible selectivity $s \approx 0.00$
\end{itemize}

The selectivity decreases exponentially with increasing potential magnitude, consistent with the Boltzmann factor $\exp(-q \Delta \phi / kT)$. The gray annotation notes: ``Electric field geometry defines aperture shape''—the spatial distribution of the electric potential creates a geometric boundary in phase space that acts as a partition.

This demonstrates that partition boundaries need not be physical walls or membranes but can be created by fields (electric, magnetic, gravitational) that modify the energy landscape. The partition is defined by the level sets of the potential energy: regions with energy below a threshold are ``inside'' the aperture, while regions with energy above the threshold are ``outside.''

\subsubsection{Zero Information Selection}

Figure~\ref{fig:partition_experiments}(F) shows a histogram of selectivity per trial across multiple experimental runs. The distribution is approximately uniform with mean $\bar{s} = 0.277$ (red line), indicating that the selectivity varies randomly from trial to trial with no systematic bias.

The green annotation emphasizes the key result:
\begin{itemize}
    \item ``No probability distribution updated''
    \item ``No wavefunction collapse''
    \item ``No Landauer erasure''
\end{itemize}

This demonstrates that geometric partition operations do not require information processing, measurement, or erasure. The partition simply divides phase space based on geometric criteria, and molecules either pass through or are blocked based on their configuration. No information about individual molecules is acquired or erased, so there is no entropy cost associated with the partition operation itself (in contrast to Maxwell's demon, which requires information acquisition and erasure, incurring an entropy cost of at least $k \ln 2$ per bit).

The zero-information nature of geometric partitions is a key feature distinguishing them from measurement-based selection. Geometric partitions are passive—they do not interact with the system beyond creating a boundary. Measurement-based selection is active—it acquires information about the system and uses that information to make selection decisions, incurring an entropy cost due to Landauer's principle.

\subsection{Independence from Oscillatory and Categorical Concepts}

We emphasize that the derivation of the partition entropy formula $\Spart = \kB M \ln n$ has proceeded entirely within the framework of partition operations and tree combinatorics, with no reference to oscillatory dynamics or categorical structure. The derivation relies solely on the following principles:

\begin{enumerate}
    \item \textbf{Partition existence} (Axiom~\ref{axiom:partition_exist}): Systems with structure can be divided into disjoint, exhaustive subsystems.
    
    \item \textbf{Constant branching factor} (Axiom~\ref{axiom:branching}): Each partition operation divides a system into $n$ parts, where $n$ is determined by the system's structure.
    
    \item \textbf{Recursive partitionability} (Axiom~\ref{axiom:recursive_part}): Each subsystem can be further partitioned using the same procedure, yielding a self-similar hierarchical structure.
    
    \item \textbf{Boltzmann-Shannon entropy relation} $S = \kB \ln W$: Entropy is the logarithm of the number of accessible states, connecting information theory to thermodynamics.
\end{enumerate}

No reference has been made to:
\begin{itemize}
    \item Oscillatory dynamics, phase space trajectories, Poincaré recurrence, or Hamiltonian mechanics
    \item Categorical distinctions, dimensional decomposition, or completion precedence
    \item Quantum mechanics, energy levels, or zero-point motion
    \item Temporal evolution, dynamical systems, or time-dependent processes (except in the discussion of partition history)
\end{itemize}

The entropy $\Spart = \kB M \ln n$ arises purely from the combinatorics of partition trees—the counting of distinguishable paths through a hierarchical branching structure. This combinatorial derivation establishes the partition perspective as an independent foundation for thermodynamic entropy, distinct from both the oscillatory perspective (Section~\ref{sec:oscillatory}) and the categorical perspective (Section~\ref{sec:categorical}).

The remarkable fact is that all three derivations yield identical formulas:
\begin{align}
    \Sosc &= \kB M \ln n \quad \text{(oscillatory mechanics)} \\
    \Scat &= \kB M \ln n \quad \text{(categorical structure)} \\
    \Spart &= \kB M \ln n \quad \text{(partition operations)}
\end{align}

This triple identity is not coincidental but reveals a fundamental equivalence: oscillation, categorization, and partition are three perspectives on a single underlying structure. The proof of this equivalence—the demonstration that the three frameworks are mathematically isomorphic and physically equivalent—is the subject of Section~\ref{sec:unification}.


