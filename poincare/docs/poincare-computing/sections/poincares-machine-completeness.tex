\section{Computational Completeness: Trajectory Equivalence}
\label{sec:completeness}

The question of computational completeness is central to any proposed computational framework: what class of problems can the framework solve, and how does this class relate to the problems solvable by established models such as Turing machines? For over eight decades, Turing completeness has served as the gold standard for computational universality \citep{turing1936computable}: a system is considered computationally complete if it can simulate any Turing machine, thereby computing any computable function. This criterion has been successfully applied to diverse computational models including lambda calculus, cellular automata, register machines, and modern programming languages.

However, Turing completeness is predicated on a specific computational paradigm: \textbf{algorithmic computation}, in which computation consists of executing a finite sequence of explicit instructions that transform input data into output data through deterministic state transitions. This paradigm assumes that the algorithm—the sequence of instructions—is the computation, and that two systems computing the same function via different algorithms are fundamentally different computational objects.

Poincaré Computing operates in a fundamentally different paradigm: \textbf{trajectory-based computation}, in which computation consists of continuous evolution through phase space guided by constraints, with solutions characterized by recurrence rather than termination. This section establishes that Poincaré Computing is \textit{categorically distinct} from Turing computation: it is neither Turing complete nor subsumed by Turing computation, and Turing machines are not trajectory-complete. The two frameworks are incomparable, operating on different mathematical objects (continuous trajectories vs. discrete instruction sequences) with different equivalence relations (answer equivalence vs. algorithmic equivalence).

We develop an alternative completeness criterion called \textbf{trajectory completeness}, which characterizes the expressiveness of Poincaré Computing in terms appropriate to its geometric structure. We prove that the categorical dynamics are trajectory-complete (Theorem~\ref{thm:trajectory_complete}), establishing that Poincaré Computing can solve any problem that admits a recurrent trajectory solution. We introduce the notion of \textbf{answer equivalence}, which identifies problems by their outputs rather than their algorithmic structure, and prove that answer-equivalent problems can have entirely disjoint trajectories and constraint structures (Theorem~\ref{thm:non_algo_equiv}). We establish the \textbf{representation-solution gap}, which formalizes the observation that perfect problem encodings paradoxically make computation unnecessary (Theorem~\ref{thm:representation_gap}).

The central philosophical claim of this section is that \textit{computation without algorithms} is not only possible but represents a distinct and valid computational paradigm. In Poincaré Computing, there is no instruction sequence, no program counter, no discrete state transitions—only continuous dynamics, constraint satisfaction, and recurrence. The fundamental invariant is the answer, not the procedure.

\subsection{The Algorithmic Assumption}

We begin by making explicit the foundational assumptions underlying Turing computation and algorithmic thinking more broadly. These assumptions are so deeply embedded in computer science that they are often taken as definitional of computation itself, yet they represent a specific choice about what computation means.

\begin{axiom}[Algorithmic Computation]
\label{ax:algorithmic}
Computation in the Turing paradigm consists of four essential components:
\begin{enumerate}
    \item \textbf{Explicit instructions:} A finite set of rules (the transition function $\delta: Q \times \Gamma \to Q \times \Gamma \times \{L, R\}$ for a Turing machine) that specify exactly how the state evolves at each step. The algorithm is an explicit, finite description of the computational process.
    
    \item \textbf{Uninformed input:} The input encoding on the tape contains no information about the output or the solution structure. The input is "raw data" that must be processed by the algorithm to produce the output.
    
    \item \textbf{Deterministic transitions:} From each configuration, there is exactly one successor configuration (in deterministic Turing machines) or a well-defined probability distribution over successors (in probabilistic variants). The computation follows a unique path through configuration space.
    
    \item \textbf{Termination criterion:} Successful computation is defined by reaching a designated halting state (accept or reject). The computation "stops" when the answer is found, and the final tape contents encode the output.
\end{enumerate}
\end{axiom}

These four assumptions define what we call the \textit{algorithmic paradigm}. They are not mathematical necessities but design choices that have proven extraordinarily successful for discrete, symbolic computation.

\begin{definition}[Turing Completeness]
\label{def:turing_complete}
A computational system $\mathcal{S}$ is \textbf{Turing complete} if for every Turing machine $M$ with input alphabet $\Sigma$ and output alphabet $\Gamma$, there exists an encoding $\iota_M$ such that for every input $x \in \Sigma^*$:
\begin{equation}
M(x) = y \in \Gamma^* \implies \mathcal{S}(\iota_M(x)) = y
\label{eq:turing_complete}
\end{equation}
In other words, $\mathcal{S}$ can simulate the input-output behavior of any Turing machine \citep{sipser2012introduction}.
\end{definition}

Turing completeness is a powerful universality criterion because it establishes that a system can compute anything computable in the Church-Turing sense. However, it is important to recognize that Turing completeness is defined in terms of \textit{input-output behavior}, not computational mechanism. Two systems can be Turing complete while using entirely different internal representations and dynamics.

\begin{remark}[Algorithm as Computation]
\label{rem:algorithm_as_computation}
A crucial property of Turing computation is that the algorithm—the transition function $\delta$—\textit{is} the computation. Two Turing machines computing the same function $f: \Sigma^* \to \Gamma^*$ via different transition functions are considered different computational objects, even though they produce identical input-output mappings. This is reflected in complexity theory, where the running time and space usage depend on the specific algorithm, not just the function being computed. The algorithmic paradigm thus identifies computation with procedure rather than with result.
\end{remark}

\subsection{Poincaré Computing: A Non-Algorithmic Framework}

We now establish formally that Poincaré Computing does not satisfy the algorithmic assumptions of Axiom~\ref{ax:algorithmic}. This is not a deficiency but a fundamental difference in computational paradigm.

\begin{theorem}[Non-Algorithmic Structure]
\label{thm:non_algorithmic}
Poincaré Computing violates each of the four conditions of Axiom~\ref{ax:algorithmic}:
\begin{enumerate}
    \item \textbf{Implicit instructions:} There is no finite set of explicit instructions. Instead, the constraint predicate $\mathcal{C}: \Sspace^* \to \{\text{true}, \text{false}\}$ specifies which trajectories are valid, but does not prescribe how to construct them.
    
    \item \textbf{Informed initial state:} The initial state $\Scoord_0$ may encode partial information about the solution structure through its position in S-entropy space and the structure of the constraint set $\mathcal{C}$.
    
    \item \textbf{Non-deterministic trajectories:} Multiple distinct trajectories can satisfy the same constraints from the same initial state (Theorem~\ref{thm:non_uniqueness}), violating deterministic uniqueness.
    
    \item \textbf{Recurrence criterion:} Successful computation is defined by recurrence $\|\gamma(T) - \Scoord_0\| < \epsilon$ (return to origin), not by termination in a designated halting state. The system completes a cycle rather than stopping.
\end{enumerate}
\end{theorem}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/unified_category_panel.png}
\caption{\textbf{The Unified Category: Point $\equiv$ Nothing $\equiv$ Singularity.} 
\textbf{(A) Dimensional Equivalence: All Three Are 0D:} Three circles (teal, purple, orange) representing Point, Nothing, and Singularity, connected by equivalence symbols ($\equiv$). All three are zero-dimensional objects with no internal structure. This demonstrates dimensional equivalence: Point (geometric object with zero extent), Nothing (absence of content), and Singularity (cosmological boundary) are the same 0D entity viewed through different lenses.
\textbf{(B) Categorical Equivalence: No Internal Structure:} Three boxes labeled Point, Nothing, and Singularity, each containing empty set symbol $\varnothing$. Caption: ``Zero Internal Distinctions''. The third box shows ``Expansion'' arrow, indicating that singularity can expand into universe. This demonstrates categorical equivalence: all three objects have empty internal category (no distinguishable parts), making them categorically identical.
\textbf{(C) Topological Equivalence: Circling Point = Circling Nothing:} Two circles (blue outlines) with centers marked by filled circle (left, ``Around Point'') and empty circle (right, ``Around Nothing''). Caption: ``Both Create Same Topology''. Equivalence symbol ($=$) connects them. This demonstrates topological equivalence: a loop encircling a point creates the same topology (fundamental group $\pi_1 = \mathbb{Z}$) as a loop encircling nothing. The distinction between point and nothing is topologically meaningless.
\textbf{(D) Category Filling Toward Singularity:} Four nested rectangles (pink shaded) decreasing in size from top to bottom, with vertical red line connecting centers. Bottom rectangle contains orange circle. Caption: ``Categories Filling''. Arrow labeled ``Completion'' points downward. This demonstrates category filling: as categorical completion proceeds, the space of incomplete states shrinks, asymptotically approaching singularity (the unique complete state). The funnel shape represents convergence toward categorical closure.
\textbf{(E) Cyclic Recurrence from Categorical Necessity:} Polar plot shows cyclic trajectory (red curve) passing through five colored circles (green, yellow, orange) at angles $90°$, $0°$, $270°$, $180°$, $45°$. The trajectory forms a closed loop, demonstrating Poincaré recurrence: categorical dynamics are cyclic, returning arbitrarily close to initial state. This validates the recurrence theorem (Theorem~\ref{thm:poincare_recurrence_formal}): categorical trajectories exhibit periodic behavior.
\textbf{(F) The Eternal Cosmic Cycle: Big Bang $\leftrightarrow$ Singularity:} Figure-eight diagram (red curve) with three labeled points: Singularity (left, orange circle), Big Bang (center crossing), Heat Death (right, green circle). Caption: ``Eternal Cycle''.}
\label{fig:unified_category}
\end{figure}

\begin{proof}
We establish each violation explicitly.

\textbf{(1) Implicit instructions:} In Poincaré Computing, the "instructions" are not explicit rules but are encoded implicitly in the constraint predicate $\mathcal{C}$ and the dynamics~\eqref{eq:dsk_dt}--\eqref{eq:dse_dt}. The constraint predicate specifies \textit{what} properties a valid trajectory must satisfy (e.g., "the trajectory must pass through region $R$ at some time $t$"), but it does not specify \textit{how} to construct such a trajectory. The dynamics evolve the state continuously according to differential equations, not through discrete instruction execution. There is no instruction pointer, no program counter, no fetch-decode-execute cycle.

Formally, a Turing machine has a finite description as a tuple $(Q, \Sigma, \Gamma, \delta, q_0, q_{\text{accept}}, q_{\text{reject}})$ where the transition function $\delta$ is a finite table. In Poincaré Computing, the analogous description is $(\Scoord_0, \mathcal{C}, \epsilon)$, where $\mathcal{C}$ is a predicate on trajectories (potentially infinite-dimensional objects) rather than a finite table of transitions. The constraint $\mathcal{C}$ is declarative (specifying properties) rather than imperative (specifying actions).

\textbf{(2) Informed initial state:} In Turing computation, the input tape contains only the problem instance (e.g., the graph to be colored, the formula to be satisfied), with no information about the solution. The algorithm must discover the solution through computation.

In Poincaré Computing, the initial state $\Scoord_0$ is chosen to encode the problem structure, and the constraint set $\mathcal{C}$ is constructed to filter trajectories that satisfy the problem requirements. If the constraint $\mathcal{C}$ is satisfiable, then by construction, the structure of $\mathcal{C}$ encodes properties of valid solutions. For example, if $\mathcal{C}$ requires the trajectory to pass through a specific region $R \subset \Sspace$, then the location of $R$ encodes information about where solutions exist in phase space.

More fundamentally, the Unknowability Theorem (Section~\ref{sec:complexity}) establishes that the true initial state $\Scoord_0$ is not directly observable but is inferred from the recurrent trajectory. This means that the "input" is not fully specified until the computation completes, creating a circular dependence between input and output that violates the uninformed input assumption.

\textbf{(3) Non-deterministic trajectories:} Theorem~\ref{thm:non_uniqueness} establishes that for generic problems $P = (\Scoord_0, \mathcal{C}, \epsilon)$ with $\epsilon > 0$, the solution set $\mathcal{A}(P)$ contains uncountably many distinct trajectories. These trajectories all start from the same initial state $\Scoord_0$, satisfy the same constraints $\mathcal{C}$, and return within tolerance $\epsilon$, yet they trace different paths through $\Sspace$.

This multiplicity arises from the continuous nature of the dynamics: the set of initial velocities $\dot{\gamma}(0) \in T_{\Scoord_0}\Sspace$ that lead to recurrent trajectories has a positive measure, and each distinct initial velocity generates a distinct trajectory. This violates the deterministic uniqueness of Turing machines, where each configuration has exactly one successor.

\textbf{(4) Recurrence vs. termination:} In Turing computation, a successful computation terminates in a halting state $q_{\text{accept}}$, at which point the tape contents encode the output. The computation "stops," and the machine no longer evolves.

In Poincaré Computing, successful computation is defined by the recurrence condition $\|\gamma(T) - \Scoord_0\| < \epsilon$: the trajectory returns to (near) its starting point. The system does not halt in a designated state but completes a cycle. After recurrence, the dynamics continue to evolve (potentially producing additional recurrences), so there is no notion of "stopping." The output is extracted from the trajectory structure via the output projection $\pi_{\text{out}}$ (Definition~\ref{def:output_projection}), not from a final state.

This difference reflects a fundamental distinction: Turing machines operate in an open-ended state space (the tape can grow arbitrarily), while Poincaré Computing operates in a bounded phase space $\Sspace = [0,1]^3$ where trajectories must eventually return to their starting regions by Poincaré's recurrence theorem.
\end{proof}

\begin{remark}[Paradigm Shift]
\label{rem:paradigm_shift}
Theorem~\ref{thm:non_algorithmic} establishes that Poincaré Computing represents a genuine paradigm shift in computational thinking, not merely a different implementation of algorithmic computation. The violations of Axiom~\ref{ax:algorithmic} are not bugs to be fixed but are essential features of the trajectory-based paradigm. Asking whether Poincaré Computing is Turing complete is analogous to asking whether a differential equation is Turing complete: the question applies a criterion from one mathematical framework to a fundamentally different one.
\end{remark}

\subsection{Answer Equivalence}

Having established that Poincaré Computing does not fit the algorithmic paradigm, we now develop an alternative equivalence relation appropriate to the trajectory-based framework. This equivalence relation identifies problems by their outputs rather than their algorithmic structure.

\begin{definition}[Output Projection]
\label{def:output_projection}
For a trajectory $\gamma: [0, T] \to \Sspace$ solving a problem $P$, the \textbf{output projection} is a function $\pi_{\text{out}}: C([0,T], \Sspace) \to \mathcal{Y}$ that extracts the computational result from the trajectory. The output space $\mathcal{Y}$ depends on the problem domain (e.g., $\mathcal{Y} = \{0,1\}^*$ for Boolean functions, $\mathcal{Y} = \mathbb{R}$ for numerical optimization, $\mathcal{Y} = \text{Graphs}$ for graph problems).

The output projection may depend on:
\begin{itemize}
    \item The final state $\gamma(T)$ (e.g., $\pi_{\text{out}}(\gamma) = \Sk(T)$);
    \item Intermediate states along the trajectory (e.g., $\pi_{\text{out}}(\gamma) = \arg\max_{t \in [0,T]} \Sk(t)$);
    \item Global trajectory properties (e.g., $\pi_{\text{out}}(\gamma) = \int_0^T \Sk(t) dt$);
    \item Discrete events along the trajectory (e.g., $\pi_{\text{out}}(\gamma) = |\{t : \gamma(t) \in R\}|$ for some region $R$).
\end{itemize}
\end{definition}

The output projection plays the role of the "tape reading" operation in Turing machines: it extracts the answer from the computational state. However, unlike Turing machines where the output is simply the final tape contents, the output projection in Poincaré Computing can be a complex functional of the entire trajectory.

\begin{definition}[Answer Equivalence]
\label{def:answer_equivalence}
Two problems $P_1 = (\Scoord_0^{(1)}, \mathcal{C}_1, \epsilon_1)$ and $P_2 = (\Scoord_0^{(2)}, \mathcal{C}_2, \epsilon_2)$ are \textbf{answer-equivalent}, written $P_1 \sim_A P_2$, if they produce the same output for all satisfying trajectories:
\begin{equation}
\forall \gamma_1 \in \mathcal{A}(P_1), \forall \gamma_2 \in \mathcal{A}(P_2) : \pi_{\text{out}}(\gamma_1) = \pi_{\text{out}}(\gamma_2)
\label{eq:answer_equivalence}
\end{equation}

In other words, two problems are answer-equivalent if every solution to the first problem produces the same output as every solution to the second problem.
\end{definition}

\begin{proposition}[Answer Equivalence is an Equivalence Relation]
\label{prop:equiv_relation}
The relation $\sim_A$ is an equivalence relation on the space of problems: it is reflexive, symmetric, and transitive.
\end{proposition}

\begin{proof}
We verify the three properties of an equivalence relation.

\textbf{Reflexivity:} For any problem $P$, we have $P \sim_A P$ because $\pi_{\text{out}}(\gamma) = \pi_{\text{out}}(\gamma)$ for any trajectory $\gamma \in \mathcal{A}(P)$. This is trivially true.

\textbf{Symmetry:} If $P_1 \sim_A P_2$, then by definition, $\pi_{\text{out}}(\gamma_1) = \pi_{\text{out}}(\gamma_2)$ for all $\gamma_1 \in \mathcal{A}(P_1)$ and $\gamma_2 \in \mathcal{A}(P_2)$. By symmetry of equality, $\pi_{\text{out}}(\gamma_2) = \pi_{\text{out}}(\gamma_1)$, so $P_2 \sim_A P_1$.

\textbf{Transitivity:} If $P_1 \sim_A P_2$ and $P_2 \sim_A P_3$, then:
\begin{align}
\pi_{\text{out}}(\gamma_1) &= \pi_{\text{out}}(\gamma_2) \quad \forall \gamma_1 \in \mathcal{A}(P_1), \gamma_2 \in \mathcal{A}(P_2) \\
\pi_{\text{out}}(\gamma_2) &= \pi_{\text{out}}(\gamma_3) \quad \forall \gamma_2 \in \mathcal{A}(P_2), \gamma_3 \in \mathcal{A}(P_3)
\end{align}
By transitivity of equality, $\pi_{\text{out}}(\gamma_1) = \pi_{\text{out}}(\gamma_3)$ for all $\gamma_1 \in \mathcal{A}(P_1)$ and $\gamma_3 \in \mathcal{A}(P_3)$, so $P_1 \sim_A P_3$.

Therefore, $\sim_A$ is an equivalence relation, partitioning the space of problems into equivalence classes of answer-equivalent problems.
\end{proof}

\subsection{Non-Algorithmic Equivalence}

The central result of this section is that answer equivalence does not imply any form of algorithmic or geometric similarity. Problems can produce identical outputs while having completely different initial states, constraint structures, and trajectory geometries.

\begin{theorem}[Non-Algorithmic Equivalence]
\label{thm:non_algo_equiv}
There exist problems $P_1 = (\Scoord_0^{(1)}, \mathcal{C}_1, \epsilon_1)$ and $P_2 = (\Scoord_0^{(2)}, \mathcal{C}_2, \epsilon_2)$ such that:
\begin{enumerate}
    \item $P_1 \sim_A P_2$ (answer-equivalent: same output)
    \item $\Scoord_0^{(1)} \neq \Scoord_0^{(2)}$ (different initial states)
    \item $\mathcal{C}_1 \neq \mathcal{C}_2$ (different constraint structures)
    \item For any $\gamma_1 \in \mathcal{A}(P_1)$ and $\gamma_2 \in \mathcal{A}(P_2)$, the trajectory images are disjoint except possibly at the output extraction point: $\gamma_1([0,T_1]) \cap \gamma_2([0,T_2]) = \emptyset$ (or has measure zero)
\end{enumerate}
\end{theorem}

\begin{proof}
We construct an explicit example demonstrating all four properties.

Let $y^* \in \mathcal{Y}$ be a target output value (e.g., $y^* = 42$ for a numerical problem). We construct two problems that both produce output $y^*$ but are otherwise completely different.

\textbf{Problem $P_1$:} 
\begin{itemize}
    \item Initial state: $\Scoord_0^{(1)} = (0.1, 0.2, 0.3)$ (lower-left region of $\Sspace$)
    \item Constraint: $\mathcal{C}_1$ requires the trajectory to pass through the region $R_1 = [0.1, 0.2] \times [0.2, 0.3] \times [0.3, 0.4] \subset \Sspace$ at some time $t_1 \in (0, T_1)$ before returning to $\Scoord_0^{(1)}$
    \item Output projection: $\pi_{\text{out}}(\gamma_1) = f_1(\gamma_1)$ where $f_1$ is designed such that any trajectory satisfying $\mathcal{C}_1$ produces output $y^*$
\end{itemize}

\textbf{Problem $P_2$:}
\begin{itemize}
    \item Initial state: $\Scoord_0^{(2)} = (0.8, 0.7, 0.6)$ (upper-right region of $\Sspace$, disjoint from $\Scoord_0^{(1)}$)
    \item Constraint: $\mathcal{C}_2$ requires the trajectory to pass through the region $R_2 = [0.7, 0.8] \times [0.6, 0.7] \times [0.5, 0.6] \subset \Sspace$ at some time $t_2 \in (0, T_2)$ before returning to $\Scoord_0^{(2)}$
    \item Output projection: $\pi_{\text{out}}(\gamma_2) = f_2(\gamma_2)$ where $f_2$ is designed such that any trajectory satisfying $\mathcal{C}_2$ produces output $y^*$
\end{itemize}

\textbf{Verification of properties:}

(1) \textit{Answer equivalence:} By construction, both $f_1$ and $f_2$ are designed to produce output $y^*$ for any satisfying trajectory. Therefore, $\pi_{\text{out}}(\gamma_1) = y^* = \pi_{\text{out}}(\gamma_2)$ for all $\gamma_1 \in \mathcal{A}(P_1)$ and $\gamma_2 \in \mathcal{A}(P_2)$, establishing $P_1 \sim_A P_2$.

(2) \textit{Different initial states:} By construction, $\Scoord_0^{(1)} = (0.1, 0.2, 0.3) \neq (0.8, 0.7, 0.6) = \Scoord_0^{(2)}$.

(3) \textit{Different constraints:} The constraint $\mathcal{C}_1$ requires passing through $R_1$, while $\mathcal{C}_2$ requires passing through $R_2$. Since $R_1 \cap R_2 = \emptyset$ (the regions are disjoint), the constraints are structurally different: no trajectory can satisfy both simultaneously.

(4) \textit{Disjoint trajectories:} Any trajectory $\gamma_1 \in \mathcal{A}(P_1)$ must start at $\Scoord_0^{(1)} \in [0, 0.2]^3$ (approximately), pass through $R_1 \subset [0.1, 0.4]^3$, and return to $\Scoord_0^{(1)}$. By continuity, the trajectory remains in a neighborhood of these regions. Similarly, any trajectory $\gamma_2 \in \mathcal{A}(P_2)$ remains in a neighborhood of $[0.5, 0.8]^3$. Since these neighborhoods are disjoint (separated by a distance of at least $0.1$ in each coordinate), the trajectory images are disjoint: $\gamma_1([0,T_1]) \cap \gamma_2([0,T_2]) = \emptyset$.

This construction demonstrates that answer-equivalent problems can be geometrically and structurally completely different, with no overlap in their solution trajectories.
\end{proof}

\begin{corollary}[Path Independence]
\label{cor:path_independence}
The computational "algorithm" (trajectory geometry and constraint structure) is not an invariant of the computation. Only the output is invariant under answer equivalence.
\end{corollary}

\begin{proof}
Theorem~\ref{thm:non_algo_equiv} establishes that answer-equivalent problems $P_1 \sim_A P_2$ can have completely different trajectory geometries (disjoint images) and constraint structures ($\mathcal{C}_1 \neq \mathcal{C}_2$). Therefore, the trajectory and constraints—which in the algorithmic paradigm would constitute the "algorithm"—are not invariants of the equivalence relation. Only the output $\pi_{\text{out}}(\gamma)$ is invariant.

This contrasts sharply with algorithmic equivalence in Turing computation, where two machines are considered equivalent only if they have the same transition function (or isomorphic transition functions), making the algorithm itself the primary invariant.
\end{proof}

\begin{example}[Arithmetic Reformulation]
\label{ex:arithmetic}
Consider the problem of computing the value 1000. This can be formulated in infinitely many ways:
\begin{itemize}
    \item $P_1$: Initial state encoding "$10^3$" with constraints requiring exponentiation dynamics
    \item $P_2$: Initial state encoding "$995 + 5$" with constraints requiring addition dynamics
    \item $P_3$: Initial state encoding "$2000 / 2$" with constraints requiring division dynamics
    \item $P_4$: Initial state encoding "$\sqrt{1000000}$" with constraints requiring square root dynamics
    \item $P_5$: Initial state encoding "$\sum_{i=1}^{1000} 1$" with constraints requiring summation dynamics
\end{itemize}

All five problems are answer-equivalent: $P_1 \sim_A P_2 \sim_A P_3 \sim_A P_4 \sim_A P_5$, as they all produce output 1000. However, their initial states, constraint structures, and trajectory geometries are entirely distinct. In the algorithmic paradigm, these would be considered five different algorithms. In the trajectory paradigm, they are five reformulations of the same problem, distinguished only by their geometric representations in $\Sspace$.
\end{example}

\subsection{Reformulation Abundance}

We now formalize the observation that answer-equivalent problems are not rare exceptions but are abundant: every problem has uncountably many reformulations.

\begin{definition}[Problem Reformulation]
\label{def:reformulation}
A \textbf{reformulation} of problem $P = (\Scoord_0, \mathcal{C}, \epsilon)$ is any problem $P' = (\Scoord_0', \mathcal{C}', \epsilon')$ such that $P \sim_A P'$ (answer-equivalent). The equivalence class $[P]_{\sim_A} = \{P' : P' \sim_A P\}$ is the set of all reformulations of $P$.
\end{definition}

\begin{theorem}[Reformulation Abundance]
\label{thm:reformulation_abundance}
For any problem $P$ with non-empty solution set $\mathcal{A}(P) \neq \emptyset$, the equivalence class $[P]_{\sim_A}$ contains uncountably many distinct problems.
\end{theorem}

\begin{proof}
Let $P = (\Scoord_0, \mathcal{C}, \epsilon)$ be a problem with $\mathcal{A}(P) \neq \emptyset$, and let $\gamma^* \in \mathcal{A}(P)$ be a satisfying trajectory. Define $y^* = \pi_{\text{out}}(\gamma^*)$ as the output produced by this trajectory.

For any initial state $\Scoord_0' \in \Sspace$ (where $\Sspace = [0,1]^3$ is uncountable), we can construct a constraint $\mathcal{C}'$ as follows:
\begin{equation}
\mathcal{C}'(\gamma) = \begin{cases}
\text{true} & \text{if } \gamma(0) = \Scoord_0', \|\gamma(T) - \Scoord_0'\| < \epsilon', \text{ and } \pi_{\text{out}}(\gamma) = y^* \\
\text{false} & \text{otherwise}
\end{cases}
\end{equation}

This constraint is satisfiable: by Poincaré's recurrence theorem (Theorem~\ref{thm:poincare_recurrence}), almost every initial state $\Scoord_0'$ admits recurrent trajectories, and among these, we can select (via the output projection $\pi_{\text{out}}$) those that produce output $y^*$. Therefore, $\mathcal{A}(P') \neq \emptyset$ for the problem $P' = (\Scoord_0', \mathcal{C}', \epsilon')$.

By construction, every trajectory $\gamma' \in \mathcal{A}(P')$ satisfies $\pi_{\text{out}}(\gamma') = y^* = \pi_{\text{out}}(\gamma^*)$ for all $\gamma^* \in \mathcal{A}(P)$. Therefore, $P' \sim_A P$, so $P' \in [P]_{\sim_A}$.

Since this construction works for any $\Scoord_0' \in \Sspace$, and $\Sspace$ is uncountable (it is a subset of $\mathbb{R}^3$), the equivalence class $[P]_{\sim_A}$ contains uncountably many distinct problems (one for each choice of $\Scoord_0'$).
\end{proof}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/categorical_computer_panel.png}
\caption{\textbf{Categorical Computer: Navigation-Based Computation.} 
\textbf{(A) Problem Translation Pipeline:} Problems are translated through four stages: entity extraction (identifying categorical states), relation mapping (establishing partial order $\prec$), constraint compilation (encoding as completion operators $\mu$), and S-entropy manifold embedding (mapping to $\Sspace = [0,1]^3$). This pipeline implements forward translator $\mathcal{T}_{\text{in}}: \mathcal{P} \to \Sspace$ (Definition~\ref{def:categorical_compiler}). 
\textbf{(B) Navigation Strategies:} Comparison of convergence rates (blue bars, left axis) and operation counts (orange bars, right axis) for four navigation strategies. Categorical completion achieves comparable convergence rates ($\approx 0.8$--$1.0$) to classical methods but with dramatically reduced operation counts ($\approx 20$--$40$ vs. $100$--$140$ for gradient descent), demonstrating navigation efficiency over sequential execution. 
\textbf{(C) Complexity Scaling:} Algorithmic complexity vs. problem size $n$ for classical algorithms (orange/red: $O(n^2)$, $O(n \log n)$, $O(n)$) and categorical navigation (green: $O(1)$ constant complexity). Classical complexity grows polynomially or linearithmically; categorical complexity remains constant due to manifold navigation rather than sequential search (Theorem~\ref{thm:complexity_independence}). 
\textbf{(D) Task Speedup:} Speedup factors relative to classical baselines for four computational tasks. MatMul ($32 \times 32$ matrix multiplication) achieves $3.24\times$ speedup, Sort ($n = 10^4$ elements) achieves $13.49\times$ speedup (exceeding break-even threshold), while Dot Product and Search show sub-unity speedup ($0.55\times$, $0.08\times$) due to overhead dominating for simple tasks. 
\textbf{(E) Energy Efficiency:} Energy efficiency gain (operations per joule) for increasing problem sizes. MatMul $4 \times 4$ achieves $128\times$ gain, scaling to $65536\times$ for $32 \times 32$ matrices. Exponential scaling demonstrates energy advantage of navigation over sequential execution, consistent with thermodynamic efficiency of categorical dynamics (Section~\ref{sec:virtual_instrument}). 
\textbf{(F) Problem Type Success Rates:} Convergence success rates for five problem classes. Pattern matching achieves 100\% success (deterministic categorical structure), biological sequence analysis achieves 92\%, optimization 95\%, search 90\%, and constraint satisfaction 88\%. Target threshold (dashed line) is 90\%; four of five classes meet or exceed target, validating broad applicability.}
\label{fig:categorical_computer_performance}
\end{figure}

\begin{remark}[Implications for Problem Encoding]
\label{rem:encoding_implications}
Theorem~\ref{thm:reformulation_abundance} has profound implications for problem encoding. In traditional computation, there is typically a "natural" encoding of a problem (e.g., the adjacency matrix for a graph problem, the clause list for a SAT problem), and different encodings are considered variations on the same problem. In Poincaré Computing, there is no unique natural encoding: every problem has uncountably many equally valid reformulations, distinguished only by their geometric positions in $\Sspace$. The choice of reformulation affects the trajectory geometry and recurrence time but not the answer.
\end{remark}

\subsection{The Representation-Solution Gap}

We now formalise a subtle but important phenomenon: the relationship between how a problem is encoded and whether solutions exist. In traditional computation, the encoding is separate from the solution: the input contains no information about the output. In Poincaré Computing, the encoding (initial state and constraints) and the solution (recurrent trajectory) are intertwined in a way that creates a "representation-solution gap."

\begin{definition}[Encoding Fidelity]
\label{def:encoding_fidelity}
For a function $f: \mathcal{X} \to \mathcal{Y}$ (the intended computation) and a problem encoding $\iota: \mathcal{X} \to \Sspace \times \mathcal{C}$ that maps inputs to initial states and constraints, the \textbf{encoding fidelity} is:
\begin{equation}
\mathcal{F}(\iota, f) = \frac{|\{x \in \mathcal{X} : \exists \gamma \in \mathcal{A}(\iota(x)), \pi_{\text{out}}(\gamma) = f(x)\}|}{|\mathcal{X}|}
\label{eq:encoding_fidelity}
\end{equation}
This measures the fraction of inputs for which the encoding produces a problem with solutions that compute the correct output.
\end{definition}

Perfect encoding fidelity ($\mathcal{F}(\iota, f) = 1$) means that for every input $x$, there exists a satisfying trajectory that produces the correct output $f(x)$. Zero fidelity ($\mathcal{F}(\iota, f) = 0$) means that no input is correctly computed.

\begin{theorem}[Representation-Solution Gap]
\label{thm:representation_gap}
Let $f: \mathcal{X} \to \mathcal{Y}$ be the intended computation, and let $P = (\Scoord_0, \mathcal{C}, \epsilon)$ be a problem encoding an input $x \in \mathcal{X}$. Define:
\begin{itemize}
    \item $\mathcal{A}(P)$ = set of trajectories satisfying recurrence and constraints
    \item $\mathcal{S}(f, x) = \{\gamma : \pi_{\text{out}}(\gamma) = f(x)\}$ = set of trajectories producing the correct output
\end{itemize}
Then in general:
\begin{equation}
\mathcal{A}(P) \not\subseteq \mathcal{S}(f, x) \quad \text{and} \quad \mathcal{S}(f, x) \not\subseteq \mathcal{A}(P)
\label{eq:representation_gap}
\end{equation}
The intersection $\mathcal{A}(P) \cap \mathcal{S}(f, x) \neq \emptyset$ if and only if the encoding correctly computes $f(x)$.
\end{theorem}

\begin{proof}
The set $\mathcal{A}(P)$ is determined entirely by the dynamics (equations~\eqref{eq:dsk_dt}--\eqref{eq:dse_dt}), the initial state $\Scoord_0$, and the constraints $\mathcal{C}$. It is independent of the external interpretation function $f$.

The set $\mathcal{S}(f, x)$ is determined entirely by the semantic function $f$ and the output projection $\pi_{\text{out}}$. It is independent of the dynamics and constraints.

\textbf{$\mathcal{A}(P) \not\subseteq \mathcal{S}(f, x)$:} If the encoding $\iota$ does not correctly capture the function $f$, then there may exist trajectories that satisfy the constraints (and hence are in $\mathcal{A}(P)$) but produce incorrect outputs (and hence are not in $\mathcal{S}(f, x)$). For example, if the constraints are too permissive, they may allow trajectories that violate the problem requirements.

\textbf{$\mathcal{S}(f, x) \not\subseteq \mathcal{A}(P)$:} Conversely, there may exist trajectories that would compute $f(x)$ correctly (and hence are in $\mathcal{S}(f, x)$) but violate the constraints or fail to recur (and hence are not in $\mathcal{A}(P)$). For example, if the constraints are too restrictive, they may exclude valid solution trajectories.

The encoding correctly computes $f(x)$ if and only if there exists at least one trajectory that both satisfies the constraints and produces the correct output: $\mathcal{A}(P) \cap \mathcal{S}(f, x) \neq \emptyset$.
\end{proof}

\begin{corollary}[Perfect Encoding Paradox]
\label{cor:perfect_encoding}
If an encoding $\iota$ achieves perfect fidelity ($\mathcal{F}(\iota, f) = 1$), meaning that for every input $x$, the constraints $\mathcal{C}$ force all satisfying trajectories to produce the correct output $f(x)$, then the constraint structure $\mathcal{C}$ contains sufficient information to determine $f(x)$ without executing the dynamics. In this sense, perfect encoding makes computation unnecessary—the "answer" is already encoded in the problem specification.
\end{corollary}

\begin{proof}
If $\mathcal{F}(\iota, f) = 1$, then for every input $x \in \mathcal{X}$, we have $\mathcal{A}(P) \subseteq \mathcal{S}(f, x)$: every trajectory satisfying the constraints produces the correct output. This means that the constraint predicate $\mathcal{C}$ distinguishes correct outputs from incorrect outputs, which requires $\mathcal{C}$ to encode the graph of $f$.

More precisely, if $\mathcal{C}$ forces $\pi_{\text{out}}(\gamma) = f(x)$ for all $\gamma \in \mathcal{A}(P)$, then $\mathcal{C}$ must "know" the value of $f(x)$. This information is encoded in the structure of $\mathcal{C}$ (e.g., in the regions of $\Sspace$ that trajectories are required to visit). Therefore, the dynamics serve only to "discover" information that is already present in the constraint structure, making them, in some sense, redundant.

This paradox highlights a fundamental tension in Poincaré Computing: perfect encodings are informationally complete, while imperfect encodings require the dynamics to search for solutions. The optimal encoding balances these extremes.
\end{proof}

\subsection{Trajectory Completeness}

Having established that Poincaré Computing does not fit the algorithmic paradigm, we now define an appropriate completeness criterion for the trajectory-based framework.

\begin{definition}[Trajectory Completeness]
\label{def:trajectory_complete}
A dynamical system on $\Sspace$ is \textbf{trajectory-complete} if, for every constraint set $\mathcal{C}$ with a non-empty solution space $\mathcal{S}(\mathcal{C}) \neq \emptyset$, there exists an initial state $\Scoord_0 \in \Sspace$ and a tolerance $\epsilon > 0$ such that the dynamics produce a satisfying trajectory:
\begin{equation}
\forall \mathcal{C} : \mathcal{S}(\mathcal{C}) \neq \emptyset \implies \exists \Scoord_0, \epsilon : \mathcal{A}(\Scoord_0, \mathcal{C}, \epsilon) \neq \emptyset
\label{eq:trajectory_completeness}
\end{equation}
In other words, if a constraint is satisfiable (admits at least one trajectory satisfying it), then the dynamics can produce such a trajectory from some initial state.
\end{definition}

\begin{theorem}[Poincaré Trajectory Completeness]
\label{thm:trajectory_complete}
The categorical dynamics on $\Sspace$ with measure-preserving flow (satisfying the modular condition~\eqref{eq:modular_condition}) are trajectory-complete.
\end{theorem}

\begin{proof}
Let $\mathcal{C}$ be a constraint predicate with a non-empty solution space $\mathcal{S}(\mathcal{C}) \neq \emptyset$. By definition, there exists at least one trajectory $\gamma^*: [0, T^*] \to \Sspace$ satisfying $\mathcal{C}(\gamma^*) = \text{true}$.

Let $\Scoord_0 = \gamma^*(0)$ be the initial state of this trajectory, and let $\epsilon = \|\gamma^*(T^*) - \gamma^*(0)\| + \delta$ for some small $\delta > 0$. Then, by construction:
\begin{itemize}
    \item $\gamma^*(0) = \Scoord_0$ (initial condition satisfied)
    \item $\|\gamma^*(T^*) - \Scoord_0\| = \|\gamma^*(T^*) - \gamma^*(0)\| < \epsilon$ (recurrence condition satisfied)
    \item $\mathcal{C}(\gamma^*) = \text{true}$ (constraint satisfied)
\end{itemize}

Therefore, $\gamma^* \in \mathcal{A}(\Scoord_0, \mathcal{C}, \epsilon)$ establishes that $\mathcal{A}(\Scoord_0, \mathcal{C}, \epsilon) \neq \emptyset$.

This construction works for any satisfiable constraint $\mathcal{C}$: we simply choose the initial state to be the starting point of any trajectory that satisfies the constraint. By Poincaré's recurrence theorem (Theorem~\ref{thm:poincare_recurrence}), such trajectories exist for almost every initial state under measure-preserving dynamics, ensuring that the dynamics are trajectory-complete.
\end{proof}

\begin{remark}[Comparison with Turing Completeness]
\label{rem:completeness_comparison}
Trajectory completeness and Turing completeness are incomparable notions:
\begin{itemize}
    \item Turing completeness asks: "Can the system simulate any Turing machine?" (input-output equivalence)
    \item Trajectory completeness asks: "Can the system produce any satisfiable trajectory?" (constraint satisfaction)
\end{itemize}
A system can be trajectory-complete without being Turing complete (Poincaré Computing), and a system can be Turing complete without being trajectory-complete (Turing machines cannot produce continuous recurrent trajectories). The two criteria apply to different computational paradigms and are not directly comparable.
\end{remark}

\subsection{Categorical Distinction and Incomparability}

We conclude by formally establishing that Poincaré Computing and Turing computation are incomparable frameworks.

\begin{theorem}[Incomparability]
\label{thm:incomparability}
Poincaré Computing and Turing computation are incomparable computational frameworks in the following sense:
\begin{enumerate}
    \item Poincaré Computing is not Turing complete (cannot simulate arbitrary Turing machines while preserving algorithmic structure)
    \item Turing machines are not trajectory-complete (they cannot produce continuous, recurrent trajectories in a bounded phase space)
    \item Neither framework subsumes the other: there exist problems solvable in one framework but not naturally expressible in the other
\end{enumerate}
\end{theorem}

\begin{proof}
\textbf{(1) Poincaré Computing is not Turing complete:} Turing completeness requires that for every Turing machine $M$, there exists an encoding in Poincaré Computing that simulates $M$ step-by-step, preserving the algorithmic structure. However, Poincaré Computing does not execute instructions sequentially (Theorem~\ref{thm:non_algorithmic}): it evolves states continuously according to differential equations. There is no instruction pointer, no discrete state transitions, no tape head movement.

While it is possible to encode the input-output behaviour of a Turing machine as a constraint set $\mathcal{C}$ (as in Proposition~\ref{prop:encoding_sufficiency}), this encoding does not preserve the algorithmic structure. The Poincaré system does not "execute" the Turing machine's transition function; it finds a trajectory that happens to produce the same output. This is answer equivalence, not algorithmic equivalence.

Therefore, Poincaré Computing is not Turing complete in the traditional sense because it cannot simulate the algorithmic execution of Turing machines.

\textbf{(2) Turing machines are not trajectory-complete:} Turing machines operate on discrete symbol strings and execute discrete state transitions. They cannot directly produce continuous trajectories in a bounded phase space $\Sspace = [0,1]^3$. While one could discretise $\Sspace$ and simulate the dynamics numerically, this would not capture the essential continuous and recurrent nature of Poincaré Computing.

More fundamentally, Turing machines are designed to halt in accept/reject states, not to recur. A Turing machine that returns to its initial configuration has computed nothing (it has simply undone its computation). Recurrence is not a natural notion in the Turing paradigm.

Therefore, Turing machines are not trajectory-complete: they cannot produce the continuous recurrent trajectories that define computation in Poincaré Computing.

\textbf{(3) Incomparability:} The two frameworks operate on different mathematical objects (discrete symbol strings vs. continuous trajectories), use different equivalence relations (algorithmic equivalence vs. answer equivalence), and have different completeness criteria (Turing completeness vs. trajectory completeness). Problems that are natural in one framework may be awkward or inexpressible in the other.

For example:
\begin{itemize}
    \item Continuous optimization problems (e.g., finding minima of smooth functions) are natural in Poincaré Computing (as trajectories flowing toward attractors) but require discretization in Turing computation.
    \item Symbolic manipulation problems (e.g., parsing formal languages) are natural in Turing computation (as string rewriting) but require encoding as geometric constraints in Poincaré Computing.
\end{itemize}

Neither framework subsumes the other; they are complementary approaches to computation.
\end{proof}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/processor_benchmark_performance.png}
\caption{\textbf{Processor Benchmark: Categorical vs Classical Performance---$O(1)$ Categorical Completion vs $O(n)$, $O(n^2)$, $O(n^3)$ Classical.} 
\textbf{(A) Speedup by Task (Categorical/Classical):} Bar chart shows speedup factor (categorical time / classical time, log scale) for 14 tasks. Red bars (classical slower, speedup $< 1$): dot product $n = 10$ ($\approx 0.05$), dot product $n = 100$ ($\approx 0.1$), dot product $n = 1000$ ($\approx 0.5$), dot product $n = 10000$ ($\approx 0.8$), matrix multiply $4 \times 4$ ($\approx 0.1$), matrix multiply $8 \times 8$ ($\approx 0.8$), search $n = 1000$ ($\approx 0.03$), search $n = 10000$ ($\approx 0.01$), search $n = 100000$ ($\approx 0.003$).
\textbf{(B) Operation Reduction:} Grouped bar chart shows operation count (log scale) for four task types. Classical ops (red bars): Dot Product ($\approx 10^8$), Matrix Mul ($\approx 10^{13}$), Sorting ($\approx 10^{12}$), Search ($\approx 10^1$). Categorical steps (green bars): all tasks $\approx 10^0$ (single step). Reduction factors: Dot Product ($10^8\times$), Matrix Mul ($10^{13}\times$), Sorting ($10^{12}\times$), Search ($10^1\times$). Matrix multiplication achieves greatest reduction ($10^{13}\times$), confirming that categorical completion collapses polynomial-time operations to constant time.
\textbf{(C) Dot Product Scaling:} Time (ms, log scale) vs. input size (log scale) for dot product. Classical (red circles, solid line): grows linearly from $\approx 0.01$ ms at $n = 10$ to $\approx 1000$ ms at $n = 10^4$ (slope $\approx 1$, confirming $O(n)$ complexity). Categorical (green squares, solid line): remains flat at $\approx 0.5$ ms for all $n$ (confirming $O(1)$ complexity). Lines cross at $n \approx 10^3$, defining crossover point. For $n < 10^3$, classical is faster (overhead dominates); for $n > 10^3$, categorical is faster.
\textbf{(D) Matrix Multiply Scaling:} Time (ms, log scale) vs. input size (log scale) for matrix multiplication. Classical (red circles, solid line): grows cubically from $\approx 0.05$ ms at $n = 10^2$ to $\approx 5000$ ms at $n = 10^3$ (slope $\approx 3$, confirming $O(n^3)$ complexity). Categorical (green squares, solid line): grows sub-linearly from $\approx 0.5$ ms at $n = 10^2$ to $\approx 100$ ms at $n = 10^3$ (slope $\approx 0.3$, confirming $O(1)$ categorical completion with measurement overhead). Lines cross at $n \approx 200$, defining crossover.
\textbf{(F) Result Accuracy:} Pie chart shows match rate for categorical results vs. classical ground truth. Match (green): 100\%. Mismatch (white): 0\%. Perfect accuracy confirms that categorical computation produces identical results to classical algorithms, validating correctness.
\textbf{(G) Performance Profile: Best vs Worst Cases:} Horizontal bar chart shows speedup factor (log scale) for best and worst cases. BEST cases (green bars): sort $n = 10000$ ($\approx 10\times$), sort $n = 1000$ ($\approx 8\times$), matrix multiply $32 \times 32$ ($\approx 5\times$). WORST cases (red bars): search $n = 100000$ ($\approx 0.003\times$), search $n = 10000$ ($\approx 0.01\times$), search $n = 1000$ ($\approx 0.03\times$).}
\label{fig:processor_benchmark_performance}
\end{figure}

\begin{remark}[Paradigm Plurality]
\label{rem:paradigm_plurality}
Theorem~\ref{thm:incomparability} establishes that there is not one universal computational paradigm but multiple incomparable paradigms, each suited to different classes of problems. This plurality is analogous to the situation in mathematics, where different axiomatic systems (e.g., Euclidean vs. non-Euclidean geometry) are incomparable but equally valid. The existence of Poincaré Computing as a distinct paradigm enriches our understanding of what computation can be.
\end{remark}

\subsection{Implications: Computation Without Algorithms}

We conclude this section by summarising the philosophical implications of the trajectory-based paradigm.

\begin{proposition}[Computation Without Algorithms]
\label{prop:no_algorithm}
In Poincaré Computing, the notion of "algorithm" is not well-defined. Computation does not consist of executing a sequence of instructions but rather of:
\begin{itemize}
    \item Specifying an initial state $\Scoord_0$ (problem encoding)
    \item Defining constraints $\mathcal{C}$ (solution requirements)
    \item Evolving the state according to continuous dynamics~\eqref{eq:dsk_dt}--\eqref{eq:dse_dt}
    \item Detecting recurrence $\|\gamma(T) - \Scoord_0\| < \epsilon$ (solution completion)
    \item Extracting output via $\pi_{\text{out}}(\gamma)$ (answer retrieval)
\end{itemize}
There is no "algorithm" in the traditional sense—no procedure, no control flow, no instruction sequence.
\end{proposition}

\begin{proposition}[Answer Primacy]
\label{prop:answer_primacy}
The fundamental invariant of Poincaré computation is the answer, not the procedure. Two computations are equivalent ($P_1 \sim_A P_2$) if and only if they produce the same output, regardless of:
\begin{itemize}
    \item Initial state differences ($\Scoord_0^{(1)} \neq \Scoord_0^{(2)}$)
    \item Constraint structure differences ($\mathcal{C}_1 \neq \mathcal{C}_2$)
    \item Trajectory geometry differences ($\gamma_1([0,T_1]) \cap \gamma_2([0,T_2]) = \emptyset$)
    \item Recurrence time differences ($T_1 \neq T_2$)
\end{itemize}
This represents a fundamental shift from procedure-centric to result-centric computation.
\end{proposition}

This completes the formal establishment of Poincaré Computing as a computational framework categorically distinct from Turing computation. The two paradigms operate on different mathematical objects (continuous trajectories vs. discrete instruction sequences), employ different equivalence relations (answer equivalence vs. algorithmic equivalence), and satisfy different completeness criteria (trajectory completeness vs. Turing completeness). Poincaré Computing demonstrates that computation without algorithms is not only possible but also represents a coherent and complete alternative to the algorithmic paradigm that has dominated computer science for nearly a century.

